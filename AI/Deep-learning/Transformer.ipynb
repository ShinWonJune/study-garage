{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "#### Self Attention\n",
    "-  n 개의 단어가 한방에 들어감\n",
    "- n개의 단어 각각을 feature vector로 transform\n",
    "- but 단어 각각 뿐 아니라 같이 들어간 단어의 정보를 반영하여 transform! self aattention의 가장 큰 의미\n",
    "- dependent for each paths\n",
    "- __input 단어마다 3개의 Neural Network를 통해 3개의 vector를 만든들고 이를 통해 첫 단어의 embedding vector x1을 만든다.__\n",
    "    - query, Key, Value\n",
    "- 각 input 단어에 대해 Score를 도출한다. (단어1 에 대한 query vector) @ (모든 단어에 대한 key vector)\n",
    "    - 단어1 과 나머지 단어들에 대한 vector 유사도, 얼마나 관계가 있는지를 측정\n",
    "- Normalize by dim(keys)**1/2, (dim(keys) = 64 면 score를 8로 나누어줌) score value를 너무 크지않게함\n",
    "- Normalized score를 softmax 취해줌. 단어 1 과 나머지 단어들의 score가 sum to 1이 되도록. -> Attension Weight이다.\n",
    "- 이 Attension weight과 Value vector를 weight sum 해주어\n",
    "#### feed forward\n",
    "- feed forward 는 각각 독립적으로 들어간다. 서로 영향 X\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
