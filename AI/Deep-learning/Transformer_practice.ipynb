{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "- Mulit-Headded Attention\n",
    "    - Scaled Dot-Product Attension\n",
    "- Positional Encoding\n",
    "    - Vanila\n",
    "    - Pytorch efficient\n",
    "    - DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attension\n",
    "- Data $X \\in R^{n\\times d}$ whrer $n$ is the number of data and $d$ is the data dimension. Usually 512\n",
    "- $Q,K\\in R^{n\\times d_k}$, where $d_k$ is dimension of $K$.\n",
    "- $V\\in R^{n\\times d_v}$\n",
    "\n",
    "##### $\\text{Attention}(Q,K,V) = \\text{softmax}\\big(\\frac{QK^T}{\\sqrt{d_k}}\\big)V \\in \\mathbb{R}^{n\\times d_v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDPA: Q[3, 30, 128] K[3, 30, 128] V[3, 30, 258] => out[3, 30, 258] attention[3, 30, 30]\n",
      "SDPA: Q[3, 5, 30, 128] K[3, 5, 30, 128] V[3, 5, 30, 258] => out[3, 5, 30, 258] attention[3, 5, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# self attention\n",
    "class SDPA(nn.Module):\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        d_k = K.size()[-1] # key dimension\n",
    "        scores = Q.matmul(K.transpose(-2,-1))/np.sqrt(d_k)\n",
    "        if mask is not None: #if mask exists\n",
    "            scores = scores.maksed_fill(mask==0, -1e9) #mask == 0 인 부분에 -1e9\n",
    "        attention = F.softmax(scores,dim=-1)#last dim (row vector) is a query score with other keys.\n",
    "        out = attention.matmul(V)\n",
    "        return out, attention\n",
    "    \n",
    "# Demo run\n",
    "sdpa = SDPA()\n",
    "n_batch,d_k,d_v = 3,128,258\n",
    "Q_n, K_n, V_n = 30, 30, 30 # sequence length\n",
    "Q = torch.rand(n_batch,Q_n,d_k)\n",
    "K = torch.rand(n_batch,K_n,d_k)\n",
    "V = torch.rand(n_batch,Q_n,d_v)\n",
    "out, attention = sdpa(Q,K,V)\n",
    "def sh(x): return str(x.shape)[11:-1]\n",
    "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
    "\n",
    "#supports Multi-headedAttention as well\n",
    "head = 5\n",
    "Q = torch.rand(n_batch,head,Q_n,d_k)\n",
    "K = torch.rand(n_batch,head,K_n,d_k)\n",
    "V = torch.rand(n_batch,head,Q_n,d_v)\n",
    "out, attention = sdpa(Q,K,V)\n",
    "def sh(x): return str(x.shape)[11:-1]\n",
    "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
    "\n",
    "#Batch 수가 head 보다 상위 dimension. 5개의 head를 3개씩 process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (MHA)\n",
    "- In the case of splitting embeddings into the number of heads and assign to each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,devise=None):\n",
    "        \"\"\"\n",
    "        d_feat : feature dimension (same for each input words)\n",
    "        n_head : number of heads\n",
    "        actv : activation function after each linear\n",
    "        USE_BIAS : whether to use bias\n",
    "        dropout_p : dropout rate\n",
    "        device : device select\n",
    "        \"\"\"\n",
    "\n",
    "        super(MHA,self).__init__()\n",
    "        if (d_feat%n_head) != 0:\n",
    "            raise ValueError(f\"d_feat({d_feat}) should be divisible by ({n_head})\")\n",
    "        self.d_feat = d_feat\n",
    "        self.n_head = n_head\n",
    "        self.d_head = self.d_feat // self.n_head\n",
    "        self.actv = actv\n",
    "        self.USE_BIAS = USE_BIAS\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # embedding 된 latent vector 상태로 들어옴\n",
    "        self.lin_Q = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_K = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_V = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_O = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        #dropout arg 는 %로 주고, 클래스 안에서 nn.dropout 함수에 전달.\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        \"\"\"\n",
    "        Q:[n_batch,n_Q,d_feat]\n",
    "        K:[n_batch,n_K,d_feat]\n",
    "        V:[n_batch,n_V,d_feat], where n_K == n_V\n",
    "        \"\"\"\n",
    "\n",
    "        n_batch = Q.shape[0] # Q는 batch 단위로 들어온다\n",
    "        Q_feat = self.lin_Q(Q) # [n_batch, n_Q, d_feat]\n",
    "        K_feat = self.lin_K(K) \n",
    "        V_feat = self.lin_V(V)\n",
    "        \n",
    "        #It is important to shape the lower dimension first and then permutate.\n",
    "        #Actually I don't gettit.\n",
    "        Q_split = Q_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        K_split = K_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        V_split = V_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        print(\"K_shape:\",K_split.shape)\n",
    "        d_K = K_split.size()[-1] # shape이 반환하는 tuple 에서 바로 index 접근\n",
    "        #K_split.size()[-1]. .size() method를 불로온 후에 index 접근\n",
    "        \n",
    "        scores = Q_split.matmul(K_split.transpose(-2,-1))/np.sqrt(d_K)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0,-1e9)\n",
    "        attention = torch.softmax(scores,dim=-1) #dim=-1 : softmax along dimension -1 \n",
    "        print(\"attention shape:\",attention.shape)\n",
    "        # attention = [n_batch, n_head, n_Q, n_K]\n",
    "        x_raw = torch.matmul(self.dropout(attention),V_split)\n",
    "        # x_raw = [n_batch, n_head,n_Q,d_head]\n",
    "\n",
    "        # Reshape x to operate add and normalization\n",
    "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous() #x_rh1의 객체 id 순서를 dim 1 따라 해줌\n",
    "        # x_rh1 : [n_batch, n_Q, n_head, d_head]\n",
    "        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n",
    "        # x_rh2 : [n_batch,n_Q, d_feat]\n",
    "\n",
    "        # Linear\n",
    "        x = self.lin_O(x_rsh2)\n",
    "        # x:[n_batch, n_Q, d_feat]\n",
    "\n",
    "        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n",
    "               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n",
    "               'scores':scores,'attention':attention,\n",
    "               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x} # x is the output of MHA\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_shape: torch.Size([128, 5, 32, 40])\n",
      "attention shape: torch.Size([128, 5, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Self-Attention Layer\n",
    "n_batch = 128\n",
    "n_src   = 32\n",
    "d_feat  = 200\n",
    "n_head  = 5\n",
    "src = torch.rand(n_batch,n_src,d_feat)\n",
    "self_attention = MHA(\n",
    "    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1)\n",
    "out = self_attention.forward(src,src,src,mask=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "$PE(pos,2i) = \\sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE(pos,2i+1) = \\cos(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "- For each sequences, identical PE can be used by broadcasting within each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanila PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.4477,  0.8942,  0.2138,  0.9769],\n",
       "         [ 0.9093, -0.4161,  0.8006,  0.5992,  0.4177,  0.9086],\n",
       "         [ 0.1411, -0.9900,  0.9841,  0.1774,  0.6023,  0.7983],\n",
       "         [-0.7568, -0.6536,  0.9594, -0.2820,  0.7590,  0.6511]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# Basic method\n",
    "\n",
    "def gen_pe(max_length, d_model, n):\n",
    "\n",
    "    # empty matrix\n",
    "    pe = np.zeros(max_length*d_model).reshape(max_length,d_model)\n",
    "\n",
    "    # for each position\n",
    "    for k in np.arange(max_length):\n",
    "\n",
    "        #for each dimension\n",
    "        for i in np.arange(d_model//2):\n",
    "            theta = k / n**((2*i)/d_model)\n",
    "\n",
    "            pe[k,2*i] = math.sin(theta)\n",
    "            pe[k,2*i+1] = math.cos(theta)\n",
    "    pe = torch.tensor(pe).unsqueeze(0)\n",
    "    return pe\n",
    "\n",
    "max_lenth = 10\n",
    "n = 10\n",
    "d_model = 6\n",
    "\n",
    "positions = gen_pe(max_lenth, d_model, n)\n",
    "positions[:,:5] # first dimention is for batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Efficient PE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since basic function consists double loop, not efficient.\n",
    "\n",
    "divisor tranforms as following :\n",
    "\n",
    "$\\frac{1}{n^{\\frac{2i}{d_{model}}}} = n^{-\\frac{2i}{d_model}}=\\exp^{\\log(n^{-\\frac{2i}{d_{model}}})}=\\exp^{-\\frac{2i}{d_{model}}\\log(n)}=\\exp^{-\\frac{2i \\log(n)}{d_{model}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.1000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 #d_model/2 == range if i \n",
    "n = 100\n",
    "div_term = torch.exp(torch.arange(0,d_model,2)*-(math.log(n)/d_model))\n",
    "#torch.arange(0,d_model,2) = 0, 2, 4 , ... , 2i\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating K\n",
    "\n",
    "max_lenth = 10 #K\n",
    "k = torch.arange(max_lenth).reshape(-1,1)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.0998,  0.9950],\n",
       "         [ 0.9093, -0.4161,  0.1987,  0.9801],\n",
       "         [ 0.1411, -0.9900,  0.2955,  0.9553],\n",
       "         [-0.7568, -0.6536,  0.3894,  0.9211],\n",
       "         [-0.9589,  0.2837,  0.4794,  0.8776],\n",
       "         [-0.2794,  0.9602,  0.5646,  0.8253],\n",
       "         [ 0.6570,  0.7539,  0.6442,  0.7648],\n",
       "         [ 0.9894, -0.1455,  0.7174,  0.6967],\n",
       "         [ 0.4121, -0.9111,  0.7833,  0.6216]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(max_length,d_model)\n",
    "pe[:,::2] = torch.sin(k*div_term)\n",
    "pe[:,1::2] = torch.cos(k*div_term)\n",
    "\n",
    "pe = pe.unsqueeze(0) # addtional dimension for broadcasting\n",
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "-  helps with regularization and prevents neurons from co-adapting (overrelying on each other) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model : int, dropout : float = 0.1, max_length: int =5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_length,d_model)\n",
    "\n",
    "        k = torch.arange(max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*-math.log(n)/d_model)\n",
    "\n",
    "        pe[:,0::2] = torch.sin(k*div_term)\n",
    "        pe[:,1::2] = torch.cos(k*div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"pe\",pe) # save in state_dit but not a model parameter to train\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:,:x.size(1)].requires_grad_(False)\n",
    "        #.requires_grad(False) # x.size(1) = sequence x's length\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pe',\n",
       "              tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "                       [ 0.8415,  0.5403,  0.0998,  0.9950],\n",
       "                       [ 0.9093, -0.4161,  0.1987,  0.9801],\n",
       "                       [ 0.1411, -0.9900,  0.2955,  0.9553],\n",
       "                       [-0.7568, -0.6536,  0.3894,  0.9211],\n",
       "                       [-0.9589,  0.2837,  0.4794,  0.8776],\n",
       "                       [-0.2794,  0.9602,  0.5646,  0.8253],\n",
       "                       [ 0.6570,  0.7539,  0.6442,  0.7648],\n",
       "                       [ 0.9894, -0.1455,  0.7174,  0.6967],\n",
       "                       [ 0.4121, -0.9111,  0.7833,  0.6216]]]))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "max_length = 10\n",
    "dropout = 0.0\n",
    "\n",
    "# create the positional encoding matrix\n",
    "pe = PositionalEncoding(d_model, dropout, max_length)\n",
    "\n",
    "# preview the values\n",
    "pe.state_dict()\n",
    "# pe.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display attension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(attention, \n",
    "                      n_heads: int = 8, n_rows: int = 4, n_cols: int = 2):\n",
    "  \"\"\"\n",
    "    Display the attention matrix for each head of a sequence.\n",
    "\n",
    "    Args:\n",
    "        sentence:     German sentence to be translated to English; list\n",
    "        translation:  English sentence predicted by the model\n",
    "        attention:    attention scores for the heads\n",
    "        n_heads:      number of heads\n",
    "        n_rows:       number of rows\n",
    "        n_cols:       number of columns\n",
    "  \"\"\"\n",
    "  # ensure the number of rows and columns are equal to the number of heads\n",
    "  assert n_rows * n_cols == n_heads\n",
    "    \n",
    "  # figure size\n",
    "  fig = plt.figure(figsize=(15,20))\n",
    "    \n",
    "  # visualize each head\n",
    "  for i in range(n_heads):\n",
    "        \n",
    "    # create a plot\n",
    "    ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "    \n",
    "    # select the respective head and make it a numpy array for plotting\n",
    "    _attention = attention.squeeze(0)[i,:,:].cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "    # plot the matrix\n",
    "    cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "    # set the size of the labels\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # set the indices for the tick marks\n",
    "    # ax.set_xticks(range(len(sentence)))?\n",
    "    # ax.set_yticks(range(len(translation)))\n",
    "\n",
    "    # ax.set_xticklabels(sentence)\n",
    "    # ax.set_yticklabels(translation)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLIAAAGKCAYAAAAYDWieAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAApcElEQVR4nO3de5DdZYEu6rcvSefa0YRbshMkIxePCWj0DIg1A2z3qVLGAmdQCaiDo1CMloFyHEZOakQUJzAz1FGkLERGxStiETeF4mApc6QcMUb2GGqXu3R3th40JBDkkjSQdKcv6/yR3QuWHUgasvjWRz9PVZfFSje/tz5Xr3f1m9WLrkaj0QgAAAAAdLju0gEAAAAA4EAYsgAAAACogiELAAAAgCoYsgAAAACogiELAAAAgCoYsgAAAACogiELAAAAgCoYsgAAAACogiELAAAAgCoYsgAAAACoQnVD1vDwcC699NIsWbIks2fPzkknnZQf/OAHpWMVc88992TNmjVZsWJF5s6dmyOPPDJnn312BgYGSkfrGOvWrUtXV1dWrlxZOkoxP//5z3PmmWdm4cKFmTNnTlauXJlrr722dKxiNm/enHPOOSdLly7NnDlz8opXvCJXXHFFdu3aVToaHULXPEXP7J+e2UvXPEXPsD965il6Zv/0zF565inTvWe6Go1Go3SIqTj33HOzfv36fPCDH8wxxxyTL33pS7nnnnvywx/+MH/yJ39SOt4L7m1ve1vuvvvuvP3tb88JJ5yQBx98MJ/5zGfyxBNP5Kc//em0f7C7//77c9xxx6WrqytHHXVUfvGLX5SO9IL7/ve/nzPOOCOrVq3K6tWrM2/evPz617/O+Ph4/vmf/7l0vBfcli1bcsIJJ2TBggV53/vel4ULF2bDhg350pe+lDPPPDO33XZb6Yh0AF3zFD3z7PTMXrrmKXqGA6FnnqJnnp2e2UvPPEXPJGlUZOPGjY0kjauvvrp52+7duxsvf/nLGyeffHLBZOXcfffdjeHh4ZbbBgYGGn19fY13vvOdhVJ1jtWrVzfe8IY3NE499dTGihUrSsd5we3cubNx+OGHN/7iL/6iMTY2VjpOR1i3bl0jSeMXv/hFy+3nnXdeI0nj0UcfLZSMTqFrWumZZzfde6bR0DV/SM+wP3qmlZ55dnpGz/whPdNoVPWrhevXr09PT08uvPDC5m2zZs3K+eefnw0bNmTLli0F05Xx+te/PjNnzmy57ZhjjsmKFSvyy1/+slCqzvCjH/0o69evzzXXXFM6SjE33XRTtm/fnnXr1qW7uztPPvlkxsfHS8cqanBwMEly+OGHt9y+ePHidHd3T/p+YvrRNa30zDPTM3vpmlZ6hv3RM630zDPTM3vpmVZ6prL3yNq0aVOOPfbY9Pf3t9x+4oknJknuvffeAqk6T6PRyPbt23PIIYeUjlLM2NhYLrroolxwwQU5/vjjS8cp5s4770x/f3+2bt2a4447LvPmzUt/f3/e//73Z2hoqHS8Ik477bQkyfnnn5977703W7ZsyTe/+c189rOfzcUXX5y5c+eWDUhxumb/9IyeeTpd00rPsD96Zv/0jJ55Oj3TSs9UNmQ98MADWbx48aTbJ27btm3bCx2pI33961/P1q1bs3r16tJRirn++uvz29/+Np/4xCdKRylq8+bNGR0dzVve8pa88Y1vzLe+9a28973vzfXXX5/3vOc9peMV8aY3vSmf+MQn8oMf/CCrVq3KkUcemXPOOScXXXRRPvWpT5WORwfQNfunZ/TM0+maVnqG/dEz+6dn9MzT6ZlWeibpLR1gKnbv3p2+vr5Jt8+aNav559Pdr371q3zgAx/IySefnHe/+92l4xTxyCOP5KMf/Wguu+yyHHrooaXjFPXEE09k165ded/73tf8L3qcddZZ2bNnTz73uc/liiuuyDHHHFM45QvvqKOOyimnnJK3vvWtWbRoUb773e/myiuvzBFHHJE1a9aUjkdhuubZ6Rk984d0zWR6hmejZ56dntEzf0jPTDbde6aqIWv27NkZHh6edPvEywlnz579QkfqKA8++GDe/OY3Z8GCBc3fvZ+OPvKRj2ThwoW56KKLSkcpbuJ74txzz225/R3veEc+97nPZcOGDdPuQf/mm2/OhRdemIGBgSxdujTJ3iIcHx/PpZdemnPPPTeLFi0qnJKSdM0z0zN76ZlWuqaVnmF/9Mwz0zN76ZlWeqaVnqnsVwsXL16cBx54YNLtE7ctWbLkhY7UMXbu3JnTTz89O3bsyPe+971pexabN2/ODTfckIsvvjjbtm3Lfffdl/vuuy9DQ0MZGRnJfffdl0cffbR0zBfMxP3gD98I8LDDDkuSPPbYYy94ptKuu+66rFq1qvmgP+HMM8/Mrl27smnTpkLJ6BS6Zt/0zF56ZjJd00rPsD96Zt/0zF56ZjI900rPVDZkvfrVr87AwEDzXfonbNy4sfnn09HQ0FDOOOOMDAwM5Pbbb88rX/nK0pGK2bp1a8bHx3PxxRdn+fLlzY+NGzdmYGAgy5cvzxVXXFE65gvmta99bZK95/J0E++9MB1fqrx9+/aMjY1Nun1kZCRJMjo6+kJHosPomsn0zFP0zGS6ppWeYX/0zGR65il6ZjI900rPVDZkve1tb8vY2FhuuOGG5m3Dw8O58cYbc9JJJ2XZsmUF05UxNjaW1atXZ8OGDbnlllty8sknl45U1MqVK3PrrbdO+lixYkWOPPLI3HrrrTn//PNLx3zBnH322UmSL3zhCy23f/7zn09vb2/zv3gxnRx77LHZtGlTBgYGWm7/xje+ke7u7pxwwgmFktEpdE0rPdNKz0yma1rpGfZHz7TSM630zGR6ppWeSboajUajdIipOPvss3Prrbfmb/7mb3L00Ufny1/+cn72s5/l3/7t33LKKaeUjveC++AHP5hPf/rTOeOMM5rf4E/3rne9q0CqznPaaafl4Ycfzi9+8YvSUV5w559/fr74xS/m7LPPzqmnnpq77rort9xyS9auXZsrr7yydLwX3I9+9KO84Q1vyKJFi7JmzZosWrQot99+e+64445ccMEF+Zd/+ZfSEekAuuYpeubATOeeSXTN0+kZDoSeeYqeOTB6Rs9M0DNJGpXZvXt345JLLmkcccQRjb6+vsYf//EfN773ve+VjlXMqaee2kjyjB/sdeqppzZWrFhROkYRe/bsaXzsYx9rvOxlL2vMmDGjcfTRRzc+9alPlY5V1MaNGxunn35644gjjmjMmDGjceyxxzbWrVvXGBkZKR2NDqFrnqJnDsx07plGQ9f8IT3D/uiZp+iZA6Nn9MzTTfeeqe4VWQAAAABMT1W9RxYAAAAA05chCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqEK1Q9bw8HA+9rGPZXh4uHSUjuFMWjmPyZxJK+fB/riPtHIekzmTVs6jlfNgf9xHWjmPyZxJK+cx2XQ8k65Go9EoHeK5GBwczIIFC7Jz58709/eXjtMRnEkr5zGZM2nlPNgf95FWzmMyZ9LKebRyHuyP+0gr5zGZM2nlPCabjmdS7SuyAAAAAJheDFkAAAAAVKG3xEXHx8ezbdu2zJ8/P11dXc/p3zE4ONjyvziTP+Q8JnMmrV5M59FoNPL4449nyZIl6e72dxSJrmkH5zGZM2nlPFq92M5D17TSMwef85jMmbRyHpO9mM7kQHumyHtk3X///Vm2bNkLfVmAF70tW7Zk6dKlpWN0BF0D0B66Zi89A9Ae++uZIq/Imj9/fpLk7Hf8TWbM7CsRoam3b0bR60+YNXtW6QhNQ7uHSkdIkmzZ/P+VjpAk6enpjPtIp/zNZ2/vzNIRmoaGnigdIUkyOrqndISMjo7k7rv/a/Pxlae65i8v+L8zc2bZx9g9e0aKXn/CnqHOyJEky49fXjpCkuQlhywoHSFJsv1320tHSJI8su2R0hGSJDu2P1Y6QtP8hZ3xuDprTvnninv2DOWrn/9HXfO/TZzDOe/+u8ws/DPNlv/1m6LXn7Bjx0OlIzQd83+sKh0hSXLMa44pHSFJ8lCH9Mzv/ud9pSMkSX73u1+WjtA0f/7C0hGSJLNmzSkdIaOjI/nJT27db88UGbImXno7Y2Zf8R8uemd2xkgxs6/8k5MJ42OlE+zVKYNJb29n3Ec6Z8gq+0Tt6TphQOo0z/VXG16MJs5i5sxZHfAY21P4+v/beGc8jiTJrNmzS0dIksyaU/5JW5L0zeqM8yj9vGzCjBmd0zWdciblH8eeomv2eqpnOuBnmg553twpfwGcpPi4OKFT+q5THkNmzOiU+2qRKWSfOuXnzU55HEn23zOd84wWAAAAAJ6FIQsAAACAKhiyAAAAAKiCIQsAAACAKhiyAAAAAKiCIQsAAACAKhiyAAAAAKiCIQsAAACAKhiyAAAAAKjClIes4eHhXHrppVmyZElmz56dk046KT/4wQ/akQ2AaUrXANBOegagXlMesv7qr/4qn/zkJ/POd74zn/70p9PT05M/+7M/y49//ON25ANgGtI1ALSTngGoV+9UPvlnP/tZbr755lx99dW55JJLkiTnnXdeVq5cmQ9/+MP5yU9+0paQAEwfugaAdtIzAHWb0iuy1q9fn56enlx44YXN22bNmpXzzz8/GzZsyJYtWw56QACmF10DQDvpGYC6TekVWZs2bcqxxx6b/v7+lttPPPHEJMm9996bZcuWTfq64eHhDA8PN/95cHDwuWQFYBrQNQC0k54BqNuUXpH1wAMPZPHixZNun7ht27Zt+/y6q666KgsWLGh+7KsYACDRNQC0l54BqNuUhqzdu3enr69v0u2zZs1q/vm+rF27Njt37mx+eLkuAM9E1wDQTnoGoG5T+tXC2bNnt7ycdsLQ0FDzz/elr69vn2UBAH9I1wDQTnoGoG5TekXW4sWL88ADD0y6feK2JUuWHJxUAExbugaAdtIzAHWb0pD16le/OgMDA5Pe2HDjxo3NPweA50PXANBOegagblMast72trdlbGwsN9xwQ/O24eHh3HjjjTnppJO84SEAz5uuAaCd9AxA3ab0HlknnXRS3v72t2ft2rV56KGHcvTRR+fLX/5y7rvvvnzhC19oV0YAphFdA0A76RmAuk1pyEqSr3zlK7nsssvy1a9+NY899lhOOOGE3H777TnllFPakQ+AaUjXANBOegagXlMesmbNmpWrr746V199dTvyAICuAaCt9AxAvab0HlkAAAAAUIohCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAq9Ja8+OOPPp4ZM/aUjJBGo+jlmxqdEiRJV1dX6QhJkpcuOrR0hCSd9f9NJ+ik85g5a1HpCEk643tmZGS4dISO1TtzRnpnziiaYXb/nKLXnzBrzqzSEZp+ueGXpSMkSZ58fLB0hCTJwsM7o/MOWXpI6QhJkuUnLC8doWnH9h2lIyRJhp4cKh0h442x0hE60p6hkWS87OsDenrK9tyEWbPmlo7QNDbaGffXxx56rHSEJJ3xGJIkXV09pSMkSebO7S8doamvrzOeJ/b2ziwd4YB5RRYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVegtefGeGb3pmVE0Qrq6uopef0Kn5EiSRqNROkKSZHTPSOkIHaWT7iOdYrxD7qudEGNkxPdLJ5s5a2bpCEmSlxz2ktIRmrY/sKV0hCTJ73//u9IRkiTd3atKR0iSLDl6SekISZKjVhxVOkLTwNBA6QhJkuHdw6UjJN2ei+xLT09XenrKvj6g0Rgvev0Jo6Od83ykU546983uKx0hSdJb+Ofupk544pxkZGRP6QhNY2Od8X3T2zujdIQDfizziiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqjClIeuee+7JmjVrsmLFisydOzdHHnlkzj777AwMDLQrHwDTjK4BoJ30DEDdeqfyyf/0T/+Uu+++O29/+9tzwgkn5MEHH8xnPvOZvOY1r8lPf/rTrFy5sl05AZgmdA0A7aRnAOo2pSHrQx/6UG666abMnDmzedvq1atz/PHH5x//8R/zta997aAHBGB60TUAtJOeAajblIas17/+9ZNuO+aYY7JixYr88pe/PGihAJi+dA0A7aRnAOr2vN/svdFoZPv27TnkkEMORh4AmETXANBOegagHs97yPr617+erVu3ZvXq1c/4OcPDwxkcHGz5AIADpWsAaCc9A1CP5zVk/epXv8oHPvCBnHzyyXn3u9/9jJ931VVXZcGCBc2PZcuWPZ/LAjCN6BoA2knPANTlOQ9ZDz74YN785jdnwYIFWb9+fXp6ep7xc9euXZudO3c2P7Zs2fJcLwvANKJrAGgnPQNQnym92fuEnTt35vTTT8+OHTvy7//+71myZMmzfn5fX1/6+vqeU0AApiddA0A76RmAOk15yBoaGsoZZ5yRgYGB3HnnnXnlK1/ZjlwATGO6BoB20jMA9ZrSkDU2NpbVq1dnw4YNue2223LyySe3KxcA05SuAaCd9AxA3aY0ZP3t3/5tvv3tb+eMM87Io48+mq997Wstf/6ud73roIYDYPrRNQC0k54BqNuUhqx77703SfKd73wn3/nOdyb9uQd9AJ4vXQNAO+kZgLpNaci666672hQDAPbSNQC0k54BqFt36QAAAAAAcCAMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBV6S178vt/8j/T0zCgZIbt2DRa9/oSdOx8qHaFpwYLDSkdIkpz5l39ZOkKSZPfju0pHSJKM7BktHSFJ8vijj5eO0LTk5YtLR0iS9C/qLx0hQ7t35TvfKZ2iM42Ojqa7p+z3z8P3P1z0+hO2/a9tpSM0nfKW/6t0hCRJ/8L5pSMkSX5//+9LR0iSPLLtkdIRkiS3fea20hGa5r10XukISZK+WTNLR8jo8J7SETrSz+6+Mz09RX+syv/5us54TD3lrDeUjtD0X2/4SukISZJbbv5k6QhJksWLX146QpLkVa/qjPvI+/7hktIRmgbuGSgdIUny6APlnwPs2TOU/HD/n+cVWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUwZAFAAAAQBUMWQAAAABUobfkxfv7F6W3d2bJCJkzZ37R60+YPXte6QhNc+cuKB0hSbLzoR2lIyRJ9gyPlI6QJBkfHSsdIUkyNjJaOkLTzt/vLB0hSTL05FDpCBkeLp+hU42NjGWsu+z3z/Du4aLXn/DkjidLR2hatGRR6QhJkuXHLC0dIUmy6/HdpSMkSbb9+oHSEZIkDz+0rXSEpp7eZaUjJEl6entKR8joWGc8F+k0M2b0paen6I9VaYyPF73+hOEOeE40YUbhnzMnzJ7VGT/n9fXNKR0hSdLd3Rmvpemk50Sjezrk582xRukIB5yhM+5FAAAAALAfhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKz3vIWrduXbq6urJy5cqDkQcAWugZANpJzwDU5XkNWffff3+uvPLKzJ0792DlAYAmPQNAO+kZgPr0Pp8vvuSSS/K6170uY2Njefjhhw9WJgBIomcAaC89A1Cf5/yKrB/96EdZv359rrnmmoMYBwD20jMAtJOeAajTc3pF1tjYWC666KJccMEFOf744/f7+cPDwxkeHm7+8+Dg4HO5LADTxFR7JtE1ABw4PQNQr+c0ZF1//fX57W9/mzvvvPOAPv+qq67Kxz/+8edyKQCmoan2TKJrADhwegagXlP+1cJHHnkkH/3oR3PZZZfl0EMPPaCvWbt2bXbu3Nn82LJly5SDAjA9PJeeSXQNAAdGzwDUbcqvyPrIRz6ShQsX5qKLLjrgr+nr60tfX99ULwXANPRceibRNQAcGD0DULcpDVmbN2/ODTfckGuuuSbbtm1r3j40NJSRkZHcd9996e/vz8KFCw96UABe/PQMAO2kZwDqN6VfLdy6dWvGx8dz8cUXZ/ny5c2PjRs3ZmBgIMuXL88VV1zRrqwAvMjpGQDaSc8A1G9Kr8hauXJlbr311km3f+QjH8njjz+eT3/603n5y19+0MIBML3oGQDaSc8A1G9KQ9YhhxySP//zP590+zXXXJMk+/wzADhQegaAdtIzAPWb8n+1EAAAAABKmPJ/tXBf7rrrroPxrwGAfdIzALSTngGoh1dkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVegtefEZM2alt3dmyQjp6uoqev0JjUajdISmmTNnlY6QJBke2lM6QpJkbGSsdIQkyfj4eOkISZIOuqtmz/BI6QhJOuNM9gwPl47QsXpn9qZ3ZtG6y5z5c4pef8KMvhmlIzQ98JsHSkdIkux4aEfpCEmSwUcGS0dI0jn31SVHHlU6QtPs+bNLR0iS4o9jSTLeKJ+hE/3RH70qM2b0Fc3wxM4nil5/wqYfbiodoWnxkqNLR0iSLF12bOkISZLx8Q54wppkbGy0dIQkyf97052lIzR1dXt90YSRkQPbAJwYAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFXoLXnxef0LMmNGX8kI6erqKnp9ntng73eWjsC+dHfO98zI7/eUjtAxRkaGS0foWD09Penp6SmaYd6CuUWvP2HeS+eXjtC08Y6flI6QJHnsse2lIyRJlh55TOkISZKjVrysdIQkyav+86tKR2i6/3/eXzpCkuSJHU+UjpDxxljpCB1p6XFL09c3q2iGTT/+adHrT3jggd+UjtD0p//ljNIRkiSnnfufS0dIkvzqp78qHSFJsumu/1Y6QpLkv//3H5aO0LRo0X8qHSFJMm/eS0tHyOjoyAF9nldkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFAFQxYAAAAAVTBkAQAAAFCF5zRk/fznP8+ZZ56ZhQsXZs6cOVm5cmWuvfbag50NgGlM1wDQTnoGoE69U/2C73//+znjjDOyatWqXHbZZZk3b15+/etf5/77729HPgCmIV0DQDvpGYB6TWnIGhwczHnnnZc3v/nNWb9+fbq7/WYiAAeXrgGgnfQMQN2m9Kh90003Zfv27Vm3bl26u7vz5JNPZnx8vF3ZAJiGdA0A7aRnAOo2pSHrzjvvTH9/f7Zu3Zrjjjsu8+bNS39/f97//vdnaGjoGb9ueHg4g4ODLR8AsC+6BoB20jMAdZvSkLV58+aMjo7mLW95S974xjfmW9/6Vt773vfm+uuvz3ve855n/LqrrroqCxYsaH4sW7bseQcH4MVJ1wDQTnoGoG5TGrKeeOKJ7Nq1K+edd16uvfbanHXWWbn22mvz13/917n55puzefPmfX7d2rVrs3PnzubHli1bDkp4AF58dA0A7aRnAOo2pSFr9uzZSZJzzz235fZ3vOMdSZINGzbs8+v6+vrS39/f8gEA+6JrAGgnPQNQtykNWUuWLEmSHH744S23H3bYYUmSxx577CDFAmC60jUAtJOeAajblIas1772tUmSrVu3tty+bdu2JMmhhx56kGIBMF3pGgDaSc8A1G1KQ9bZZ5+dJPnCF77QcvvnP//59Pb25rTTTjtowQCYnnQNAO2kZwDq1juVT161alXe+9735otf/GJGR0dz6qmn5q677sott9yStWvXNl+mCwDPla4BoJ30DEDdpjRkJcn111+fI488MjfeeGNuvfXWvOxlL8unPvWpfPCDH2xDPACmI10DQDvpGYB6TXnImjFjRi6//PJcfvnl7cgDALoGgLbSMwD1mtJ7ZAEAAABAKYYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKrQW/LijUYjjUajZITi128a75AcSdLdVTpBkqSrx87aiTrmeybuI0/XNe4snsno6Gi6e0aLZpjdP6fo9Se87JVHlo7Q9KVP/j+lIyRJfvvb/1E6QpJkbOxNpSMkSV5x0itKR0iS/NXp/6V0hKbrdn67dIQkyc6Hd5aOkPHRsdIROtJLDntJ+mbNLprh8cFHi15/woMP/qZ0hKa+ubNKR0iS/OUpf1o6QpLkM78v/xiSJBu+92TpCEmSrVs3l47Q1Ck/X42OjpSOkLGxA3vO7icfAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCoYsAAAAAKpgyAIAAACgCr0lL77oiIWZ2TerZIT0zekrev0J8xf2l47Q9Pijg6UjJEnu/fHG0hGSJDNmdsZ9pLurp3SEJElf35zSEZoGH3+kdIQkyfDw7tIRMjo6UjpCx+rq6kpXV1fRDI3xRtHrTxgbGy8doamnZ0bpCEmS3t6ZpSMkSbq7iz4l6zhPDg+XjtA03iHfv3Sunb/fmb6+svfZ+f0Li15/wmGHvax0hKbhXZ3xOPKNDRtKR0iSbL9ve+kISTrnZ4nFi19eOkLTokX/qXSEJMm8eS8tHeGAf6bxiiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKhiwAAAAAqmDIAgAAAKAKUx6yNm/enHPOOSdLly7NnDlz8opXvCJXXHFFdu3a1Y58AExDugaAdtIzAPXqnconb9myJSeeeGIWLFiQNWvWZOHChdmwYUMuv/zy/Md//Eduu+22duUEYJrQNQC0k54BqNuUhqyvfvWr2bFjR3784x9nxYoVSZILL7ww4+Pj+cpXvpLHHnssL33pS9sSFIDpQdcA0E56BqBuU/rVwsHBwSTJ4Ycf3nL74sWL093dnZkzZx68ZABMS7oGgHbSMwB1m9KQddpppyVJzj///Nx7773ZsmVLvvnNb+azn/1sLr744sydO7cdGQGYRnQNAO2kZwDqNqVfLXzTm96UT3ziE7nyyivz7W9/u3n73//93+cf/uEfnvHrhoeHMzw83Pznib8FAYA/pGsAaCc9A1C3KQ1ZSXLUUUfllFNOyVvf+tYsWrQo3/3ud3PllVfmiCOOyJo1a/b5NVdddVU+/vGPP++wAEwPugaAdtIzAPWa0pB1880358ILL8zAwECWLl2aJDnrrLMyPj6eSy+9NOeee24WLVo06evWrl2bD33oQ81/HhwczLJly55ndABejHQNAO2kZwDqNqX3yLruuuuyatWq5gP+hDPPPDO7du3Kpk2b9vl1fX196e/vb/kAgH3RNQC0k54BqNuUhqzt27dnbGxs0u0jIyNJktHR0YOTCoBpS9cA0E56BqBuUxqyjj322GzatCkDAwMtt3/jG99Id3d3TjjhhIMaDoDpR9cA0E56BqBuU3qPrL/7u7/LHXfckT/90z/NmjVrsmjRotx+++254447csEFF2TJkiXtygnANKFrAGgnPQNQtykNWaecckp+8pOf5GMf+1iuu+66PPLII1m+fHnWrVuXD3/4w+3KCMA0omsAaCc9A1C3KQ1ZSXLiiSfmX//1X9uRBQCS6BoA2kvPANRrSu+RBQAAAAClGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAqGLIAAAAAqIIhCwAAAIAq9Ja8+NGvOSaz5swpGSFL/mhx0etPWP2615WO0HTLxo2lIyRJbvzMlaUjJEn65y8sHSFJMrNvdukISZJDDllaOkLTwMB/Kx0hSbJjx/bSEXgWjbHxNMbGi2bo7umMvzfqnVm09lsMD+8qHSFJ5+QYGRkuHSFJ0tXdVTpCkuSQ+fNLR2jq7u2M799Go1E6Qkdk6ER7hvYkjbL3k56eGUWvP2H27M753h0bGSsdIUnyyNaHS0dIkux+YnfpCEmSnp6e0hGSJHPm9JeO0NTXV3YTmTBzRl/pCOnuOrDH0s5oZgAAAADYD0MWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFUwZAEAAABQBUMWAAAAAFXoLXHRRqORJBnavavE5VvsevLJ0hGSJIODg6UjNHXKmTQa46UjJEnGx8dKR0jSOTnGxkZKR2jqlPtIJ5l4fOWps9izZ6hwkmRo9+7SEZIkuzvk8T3pnMe0TtEpj63DQ51xX+2k50Wd8Hw1SfYMl38sm3g81TV7dVLPjI52xmPI2Nho6QhNe/YMl46QJNm9qzMeQzrl8X1kZE/pCEk663lIpzwH6ITHkYkM++uZrkaBJrr//vuzbNmyF/qyAC96W7ZsydKlS0vH6Ai6BqA9dM1eegagPfbXM0WGrPHx8Wzbti3z589PV1fXc/p3DA4OZtmyZdmyZUv6+/sPcsI6OZNWzmMyZ9LqxXQejUYjjz/+eJYsWZLubr81nuiadnAekzmTVs6j1YvtPHRNKz1z8DmPyZxJK+cx2YvpTA60Z4r8amF3d/dB+1uc/v7+6v/POticSSvnMZkzafViOY8FCxaUjtBRdE37OI/JnEkr59HqxXQeuuYpeqZ9nMdkzqSV85jsxXImB9Iz/ioFAAAAgCoYsgAAAACoQrVDVl9fXy6//PL09fWVjtIxnEkr5zGZM2nlPNgf95FWzmMyZ9LKebRyHuyP+0gr5zGZM2nlPCabjmdS5M3eAQAAAGCqqn1FFgAAAADTiyELAAAAgCoYsgAAAACogiELAAAAgCoYsgAAAACogiELAAAAgCoYsgAAAACogiELAAAAgCr8/3e6pQmDkTfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(out['attention'][0],n_heads=3,n_rows=1,n_cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 4])\n",
      "K_shape: torch.Size([3, 4, 10, 1])\n",
      "attention shape: torch.Size([3, 4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "d_model = 4 #dim of embeddings\n",
    "max_length = 10 # sequence max length\n",
    "dropout = 0.1\n",
    "n = 3 # batch\n",
    "\n",
    "# embedded data\n",
    "x = torch.rand((n,max_length,d_model))\n",
    "\n",
    "# create the positional encoding matrix\n",
    "pe = PositionalEncoding(d_model, dropout, max_length)\n",
    "\n",
    "# add pe to x\n",
    "x = pe.forward(x)\n",
    "\n",
    "# attention block\n",
    "attention = MHA(d_feat = d_model, n_head = 4, actv=F.relu,USE_BIAS=True,dropout_p=0.1)\n",
    "out = attention.forward(x,x,x,mask=None) # Q,K,V as x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
