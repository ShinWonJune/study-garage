{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attension\n",
    "- Data $X \\in R^{n\\times d}$ whrer $n$ is the number of data and $d$ is the data dimension. Usually 512\n",
    "- $Q,K\\in R^{n\\times d_k}$, where $d_k$ is dimension of $K$.\n",
    "- $V\\in R^{n\\times d_v}$\n",
    "\n",
    "##### $\\text{Attention}(Q,K,V) = \\text{softmax}\\big(\\frac{QK^T}{\\sqrt{d_k}}\\big)V \\in \\mathbb{R}^{n\\times d_v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDPA: Q[3, 30, 128] K[3, 30, 128] V[3, 30, 258] => out[3, 30, 258] attention[3, 30, 30]\n",
      "SDPA: Q[3, 5, 30, 128] K[3, 5, 30, 128] V[3, 5, 30, 258] => out[3, 5, 30, 258] attention[3, 5, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# self attention\n",
    "class SDPA(nn.Module):\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        d_k = K.size()[-1] # key dimension\n",
    "        scores = Q.matmul(K.transpose(-2,-1))/np.sqrt(d_k)\n",
    "        if mask is not None: #if mask exists\n",
    "            scores = scores.maksed_fill(mask==0, -1e9) #mask == 0 인 부분에 -1e9\n",
    "        attention = F.softmax(scores,dim=-1)#last dim (row vector) is a query score with other keys.\n",
    "        out = attention.matmul(V)\n",
    "        return out, attention\n",
    "    \n",
    "# Demo run\n",
    "sdpa = SDPA()\n",
    "n_batch,d_k,d_v = 3,128,258\n",
    "Q_n, K_n, V_n = 30, 30, 30 # sequence length\n",
    "Q = torch.rand(n_batch,Q_n,d_k)\n",
    "K = torch.rand(n_batch,K_n,d_k)\n",
    "V = torch.rand(n_batch,Q_n,d_v)\n",
    "out, attention = sdpa(Q,K,V)\n",
    "def sh(x): return str(x.shape)[11:-1]\n",
    "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
    "\n",
    "#supports Multi-headedAttention as well\n",
    "head = 5\n",
    "Q = torch.rand(n_batch,head,Q_n,d_k)\n",
    "K = torch.rand(n_batch,head,K_n,d_k)\n",
    "V = torch.rand(n_batch,head,Q_n,d_v)\n",
    "out, attention = sdpa(Q,K,V)\n",
    "def sh(x): return str(x.shape)[11:-1]\n",
    "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
    "\n",
    "#Batch 수가 head 보다 상위 dimension. 5개의 head를 3개씩 process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention (MHA)\n",
    "- In the case of splitting embeddings into the number of heads and assign to each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,devise=None):\n",
    "        \"\"\"\n",
    "        d_feat : feature dimension (same for each input words)\n",
    "        n_head : number of heads\n",
    "        actv : activation function after each linear\n",
    "        USE_BIAS : whether to use bias\n",
    "        dropout_p : dropout rate\n",
    "        device : device select\n",
    "        \"\"\"\n",
    "\n",
    "        super(MHA,self).__init__()\n",
    "        if (d_feat%n_head) != 0:\n",
    "            raise ValueError(f\"d_feat({d_feat}) should be divisible by ({n_head})\")\n",
    "        self.d_feat = d_feat\n",
    "        self.n_head = n_head\n",
    "        self.d_head = self.d_feat // self.n_head\n",
    "        self.actv = actv\n",
    "        self.USE_BIAS = USE_BIAS\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # embedding 된 latent vector 상태로 들어옴\n",
    "        self.lin_Q = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_K = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_V = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        self.lin_O = nn.Linear(d_feat,d_feat,self.USE_BIAS)\n",
    "        #dropout arg 는 %로 주고, 클래스 안에서 nn.dropout 함수에 전달.\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        \"\"\"\n",
    "        Q:[n_batch,n_Q,d_feat]\n",
    "        K:[n_batch,n_K,d_feat]\n",
    "        V:[n_batch,n_V,d_feat], where n_K == n_V\n",
    "        \"\"\"\n",
    "\n",
    "        n_batch = Q.shape[0] # Q는 batch 단위로 들어온다\n",
    "        Q_feat = self.lin_Q(Q) # [n_batch, n_Q, d_feat]\n",
    "        K_feat = self.lin_K(K) \n",
    "        V_feat = self.lin_V(V)\n",
    "        \n",
    "        #It is important to shape the lower dimension first and then permutate.\n",
    "        #Actually I don't gettit.\n",
    "        Q_split = Q_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        K_split = K_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        V_split = V_feat.view(n_batch,-1,self.n_head,self.d_head).permute(0,2,1,3)\n",
    "        print(\"K_shape:\",K_split.shape)\n",
    "        d_K = K_split.size()[-1] # shape이 반환하는 tuple 에서 바로 index 접근\n",
    "        #K_split.size()[-1]. .size() method를 불로온 후에 index 접근\n",
    "        \n",
    "        scores = Q_split.matmul(K_split.transpose(-2,-1))/np.sqrt(d_K)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0,-1e9)\n",
    "        attention = torch.softmax(scores,dim=-1) #dim=-1 : softmax along dimension -1 \n",
    "        print(\"attention shape:\",attention.shape)\n",
    "        # attention = [n_batch, n_head, n_Q, n_K]\n",
    "        x_raw = torch.matmul(self.dropout(attention),V_split)\n",
    "        # x_raw = [n_batch, n_head,n_Q,d_head]\n",
    "\n",
    "        # Reshape x to operate add and normalization?\n",
    "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous() #x_rh1의 객체 id 순서를 dim 1 따라 해줌\n",
    "        # x_rh1 : [n_batch, n_Q, n_head, d_head]\n",
    "        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n",
    "        # x_rh2 : [n_batch,n_Q, d_feat]\n",
    "\n",
    "        # Linear\n",
    "        x = self.lin_O(x_rsh2)\n",
    "        # x:[n_batch, n_Q, d_feat]\n",
    "\n",
    "        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n",
    "               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n",
    "               'scores':scores,'attention':attention,\n",
    "               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x} # x is the output of MHA\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_shape: torch.Size([128, 5, 32, 40])\n",
      "attention shape: torch.Size([128, 5, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Self-Attention Layer\n",
    "n_batch = 128\n",
    "n_src   = 32\n",
    "d_feat  = 200\n",
    "n_head  = 5\n",
    "src = torch.rand(n_batch,n_src,d_feat)\n",
    "self_attention = MHA(\n",
    "    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1)\n",
    "out = self_attention.forward(src,src,src,mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embedding\n",
    "$PE(pos,2i) = \\sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE(pos,2i+1) = \\cos(pos/10000^{2i/d_{model}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PE(nn.module):\n",
    "    def __init__(self,d_model,max_len=5000):\n",
    "        super(PE,self).__init__()\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
