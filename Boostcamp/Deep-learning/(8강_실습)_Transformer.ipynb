{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsnoyeYbS7i6"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/mha.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/mha.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRwj6fIzVc-s"
      },
      "source": [
        "# Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9N9n94EVWy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c01090-2be0-43cb-ea90-316d5759c04c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print (\"device:[%s].\"%(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version:[1.12.1+cu113].\n",
            "device:[cuda:0].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRvnKZ0XlJT"
      },
      "source": [
        "### Scaled Dot-Product Attention (SDPA)\n",
        "- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension\n",
        "- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$ \n",
        "- Value $V \\in \\mathbb{R}^{n \\times d_V} $\n",
        "\n",
        "$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Z3Vd_VV5Pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf79498-d57e-464f-b76f-6c0a6adf9691"
      },
      "source": [
        "# self attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        d_K = K.size()[-1] # key dimension\n",
        "        # Q dim == K dim != V dim\n",
        "        scores = Q.matmul(K.transpose(-2,-1)) / np.sqrt(d_K)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0, -1e9)\n",
        "        attention = F.softmax(scores,dim=-1)\n",
        "        # K num == V num, \n",
        "        out = attention.matmul(V)\n",
        "        return out,attention\n",
        "\n",
        "# Demo run of scaled dot product attention \n",
        "SPDA = ScaledDotProductAttention()\n",
        "n_batch,d_K,d_V = 3,128,256 # d_K(=d_Q) does not necessarily be equal to d_V\n",
        "n_Q,n_K,n_V = 30,50,50\n",
        "Q = torch.rand(n_batch,n_Q,d_K)\n",
        "K = torch.rand(n_batch,n_K,d_K)\n",
        "V = torch.rand(n_batch,n_V,d_V)\n",
        "out,attention = SPDA.forward(Q,K,V,mask=None)\n",
        "def sh(x): return str(x.shape)[11:-1]\n",
        "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
        "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
        "\n",
        "# It supports 'multi-headed' attention\n",
        "n_batch,n_head,d_K,d_V = 3,5,128,256\n",
        "n_Q,n_K,n_V = 30,50,50 # n_K and n_V should be the same\n",
        "Q = torch.rand(n_batch,n_head,n_Q,d_K)\n",
        "K = torch.rand(n_batch,n_head,n_K,d_K)\n",
        "V = torch.rand(n_batch,n_head,n_V,d_V)\n",
        "out,attention = SPDA.forward(Q,K,V,mask=None)\n",
        "# out: [n_batch x n_head x n_Q x d_V]\n",
        "# attention: [n_batch x n_head x n_Q x n_K] \n",
        "def sh(x): return str(x.shape)[11:-1] \n",
        "print (\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
        "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n",
            "(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbi13pDi3No"
      },
      "source": [
        "### Multi-Headed Attention (MHA)\n",
        "\n",
        "$\\text{head}_{\\color{red}i} = \\text{Attention}(Q {\\color{green}W}^Q_{\\color{red}i},K {\\color{green}W}^K_{\\color{red}i}, V {\\color{green}W}^V_{\\color{red}i}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf7j24l1dnSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6570fd71-cd69-43e2-c664-5783ce2d6ae6"
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None):\n",
        "        \"\"\"\n",
        "        :param d_feat: feature dimension\n",
        "        :param n_head: number of heads\n",
        "        :param actv: activation after each linear layer\n",
        "        :param USE_BIAS: whether to use bias\n",
        "        :param dropout_p: dropout rate\n",
        "        :device: which device to use (e.g., cuda:0)\n",
        "        \"\"\"\n",
        "        super(MultiHeadedAttention,self).__init__()\n",
        "        if (d_feat%n_head) != 0:\n",
        "            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \n",
        "        self.d_feat = d_feat\n",
        "        self.n_head = n_head\n",
        "        self.d_head = self.d_feat // self.n_head\n",
        "        self.actv = actv\n",
        "        self.USE_BIAS = USE_BIAS\n",
        "        self.dropout_p = dropout_p # prob. of zeroed\n",
        "\n",
        "        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
        "        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
        "        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
        "        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
        "    \n",
        "    def forward(self,Q,K,V,mask=None):\n",
        "        \"\"\"\n",
        "        :param Q: [n_batch, n_Q, d_feat]\n",
        "        :param K: [n_batch, n_K, d_feat]\n",
        "        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n",
        "        :param mask: \n",
        "        \"\"\"\n",
        "        n_batch = Q.shape[0]\n",
        "        # 같은 input 에 대해 Q,K,V를 embedding 하므로 arg Q,K,V는 전부 같은 src가 들어오게됨\n",
        "        Q_feat = self.lin_Q(Q) \n",
        "        K_feat = self.lin_K(K) \n",
        "        V_feat = self.lin_V(V)\n",
        "        # Q_feat: [n_batch, n_Q, d_feat]\n",
        "        # K_feat: [n_batch, n_K, d_feat]\n",
        "        # V_feat: [n_batch, n_V, d_feat]\n",
        "\n",
        "        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n",
        "        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
        "        # Q_split: [n_batch, n_head, n_Q, d_head]\n",
        "        # K_split: [n_batch, n_head, n_K, d_head]\n",
        "        # V_split: [n_batch, n_head, n_V, d_head]\n",
        "\n",
        "        # Multi-Headed Attention\n",
        "        d_K = K.size()[-1] # key dimension\n",
        "        scores = Q_split.matmul(K_split.transpose(-2,-1)) / np.sqrt(d_K)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask==0,-1e9)\n",
        "        attention = torch.softmax(scores,dim=-1)\n",
        "        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\n",
        "        # attention: [n_batch, n_head, n_Q, n_K]\n",
        "        # x_raw: [n_batch, n_head, n_Q, d_head]\n",
        "\n",
        "        # Reshape x\n",
        "        # contiguous : 메모리 저장 순서를 axis = 1 방향으로 잡아줌.\n",
        "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n",
        "        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n",
        "        # 다시 차원을 축소해줌\n",
        "        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n",
        "        # x_rsh2: [n_batch, n_Q, d_feat]\n",
        "\n",
        "        # Linear\n",
        "        x = self.lin_O(x_rsh2)\n",
        "        # x: [n_batch, n_Q, d_feat]\n",
        "        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n",
        "               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n",
        "               'scores':scores,'attention':attention,\n",
        "               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n",
        "        return out\n",
        "\n",
        "# Self-Attention Layer\n",
        "n_batch = 128\n",
        "n_src   = 32\n",
        "d_feat  = 200\n",
        "n_head  = 5\n",
        "src = torch.rand(n_batch,n_src,d_feat)\n",
        "self_attention = MultiHeadedAttention(\n",
        "    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)\n",
        "out = self_attention.forward(src,src,src,mask=None)\n",
        "\n",
        "Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n",
        "Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n",
        "scores,attention = out['scores'],out['attention']\n",
        "x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n",
        "\n",
        "# Print out shapes\n",
        "def sh(_x): return str(_x.shape)[11:-1] \n",
        "print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n",
        "print ()\n",
        "print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n",
        "print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n",
        "print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n",
        "print ()\n",
        "print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n",
        "print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n",
        "print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n",
        "print ()\n",
        "print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n",
        "print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n",
        "print ()\n",
        "print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n",
        "print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n",
        "print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n",
        "print ()\n",
        "print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "\n",
            "scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
            "attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
            "\n",
            "x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
            "x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n",
            "x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
            "\n",
            "Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgqsHUT-OuJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a15ef8-95d4-4390-b1a4-5ad4806269c4"
      },
      "source": [
        "a = np.arange(100).reshape(10,10)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
              "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
              "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
              "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
              "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
              "       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
              "       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
              "       [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
              "       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
              "       [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ten2 = torch.from_numpy(a.reshape(2,10,5))\n",
        "ten2"
      ],
      "metadata": {
        "id": "ELWTKHQCBBJd",
        "outputId": "a7937b1e-517f-4215-80ec-d26f1547787a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3,  4],\n",
              "         [ 5,  6,  7,  8,  9],\n",
              "         [10, 11, 12, 13, 14],\n",
              "         [15, 16, 17, 18, 19],\n",
              "         [20, 21, 22, 23, 24],\n",
              "         [25, 26, 27, 28, 29],\n",
              "         [30, 31, 32, 33, 34],\n",
              "         [35, 36, 37, 38, 39],\n",
              "         [40, 41, 42, 43, 44],\n",
              "         [45, 46, 47, 48, 49]],\n",
              "\n",
              "        [[50, 51, 52, 53, 54],\n",
              "         [55, 56, 57, 58, 59],\n",
              "         [60, 61, 62, 63, 64],\n",
              "         [65, 66, 67, 68, 69],\n",
              "         [70, 71, 72, 73, 74],\n",
              "         [75, 76, 77, 78, 79],\n",
              "         [80, 81, 82, 83, 84],\n",
              "         [85, 86, 87, 88, 89],\n",
              "         [90, 91, 92, 93, 94],\n",
              "         [95, 96, 97, 98, 99]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ten = torch.from_numpy(a.reshape(10,2,5))\n",
        "ten.permute(1,0,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GACT5XZU-_rV",
        "outputId": "ffeabf98-c08e-4b43-bf50-4c2be8b76073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3,  4],\n",
              "         [10, 11, 12, 13, 14],\n",
              "         [20, 21, 22, 23, 24],\n",
              "         [30, 31, 32, 33, 34],\n",
              "         [40, 41, 42, 43, 44],\n",
              "         [50, 51, 52, 53, 54],\n",
              "         [60, 61, 62, 63, 64],\n",
              "         [70, 71, 72, 73, 74],\n",
              "         [80, 81, 82, 83, 84],\n",
              "         [90, 91, 92, 93, 94]],\n",
              "\n",
              "        [[ 5,  6,  7,  8,  9],\n",
              "         [15, 16, 17, 18, 19],\n",
              "         [25, 26, 27, 28, 29],\n",
              "         [35, 36, 37, 38, 39],\n",
              "         [45, 46, 47, 48, 49],\n",
              "         [55, 56, 57, 58, 59],\n",
              "         [65, 66, 67, 68, 69],\n",
              "         [75, 76, 77, 78, 79],\n",
              "         [85, 86, 87, 88, 89],\n",
              "         [95, 96, 97, 98, 99]]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**콘텐츠 라이선스**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "PeVYUcJKZqyR"
      }
    }
  ]
}