{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boot strapping\n",
    "- 데이터의 subsampling을 통해 여러 metric 또는 model을 학습하는 것.\n",
    "\n",
    "### Bagging\n",
    "- Multiple models are being trained with bootstrapping.\n",
    "- 여러 모델 결과의 평균 등을 통해 성능을 낸다. 앙상블과 비슷\n",
    "\n",
    "### Boosting\n",
    "- It focuses on thoes specific training samples that are hard to classify\n",
    "- specific train을 거친 model을 weak learner라고 함. 이들을 합쳐서 string learner를 만듬.\n",
    "- 독립적인 여러 모델을 만드는게 아니라 sequential하게 학습을 통해서 하나의 모델이 나오게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Mothods\n",
    "### Stochastic gradient descent\n",
    "- Update with the gradient computed from a single sample\n",
    "\n",
    "### Mini-Batch gradient descent\n",
    "- Update with the gradient computed from a subset of data\n",
    "\n",
    "### Batch gradient descent\n",
    "- update with the gradient computed from the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch- size matters\n",
    "- Large dataset -> Sharp minimum\n",
    "- Small dataset -> Flat minimum\n",
    "- __Flat minimum/small dataset batch__ is prefered.\n",
    "    - sharp minimun은 값이 조금만 달라져도 결과가 크게 달라질 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
