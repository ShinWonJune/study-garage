{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module\n",
    "- 딥러닝을 구성한든 Layer의 base class\n",
    "- input, output, forward, backward(AutoGrad) 정의\n",
    "- 학습의 대상이 되는 parameter(tensor) 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Parameter\n",
    "- weigh 값을 정의하는 class\n",
    "- Tensor 객체의 상속 객체\n",
    "- nn.Module 내에 attribute가 될 때는 required_grad = True으로 자동 지정되어, AutoGrad의 대상이되어 학습 대상이 되는 Tensor\n",
    "    - 또한 자동으로 module의 parameter의 list에 속하게되어 parameters() iterator에서 등장하게 됨\n",
    "    - 또한 module.state_dict() 에 자동 저장.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example ; linear function (xw+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "# layer를 구성하는 base class인 nn.Module을 상속\n",
    "class MyLiner(nn.Module):\n",
    "    # 'in_features'개의 features를 'out_features'개의 feature로.\n",
    "    # bias는 기본적으로 존재한다.\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        # nn.Modeule의 init을 기본적으로 상속\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # weight는 보통 이렇게 까지 직접 지정해주지 않는다.\n",
    "        # weight을 정의하는 nn.Parameter   \n",
    "        # nn.Parameter가 nn.Module 안에 속성이 됐으므로 자동으로 학습대상으로 지정됨(AutoGrad의 대상이 됨). required_grad = True. \n",
    "        self.weights = nn.Parameter(\n",
    "                # input features 개수 X output features의 크기를 갖는 정규분포 난수 weight 생성\n",
    "                torch.randn(in_features,out_features)\n",
    "        )\n",
    "\n",
    "        # Bias 가 True일 때, output feature 개수만큼의 bias 가짐\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    # linear function \n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 data, 7 features\n",
    "x = torch.randn(5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1239, -1.2761, -0.3707,  2.0381, -0.0659,  0.3456,  0.2790],\n",
       "        [-0.1320, -1.2445, -0.3816,  0.8433,  0.2786, -0.7823, -0.6969],\n",
       "        [-0.3531,  0.8200, -1.6397,  1.0144,  0.2942,  0.0257, -0.1580],\n",
       "        [ 0.5962,  0.1025,  0.6492,  0.0306, -0.5311,  0.7606, -1.4780],\n",
       "        [ 0.3175, -0.7066, -1.2907, -0.8492,  1.5602,  0.0400,  2.1103]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyLiner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shinw\\OneDrive\\문서\\GitHub\\study-garage\\PyTorch\\AutoGrad & Optimizer.ipynb 셀 7\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/AutoGrad%20%26%20Optimizer.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# feature를 7에서 12개로 바꿔주고 싶다.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/AutoGrad%20%26%20Optimizer.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# MyLiner에 value를 넣어주면 forward가 실행됨\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/AutoGrad%20%26%20Optimizer.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m layer \u001b[39m=\u001b[39m MyLiner(\u001b[39m7\u001b[39m,\u001b[39m12\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/AutoGrad%20%26%20Optimizer.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m layer(x)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyLiner' is not defined"
     ]
    }
   ],
   "source": [
    "# feature를 7에서 12개로 바꿔주고 싶다.\n",
    "# MyLiner에 value를 넣어주면 forward가 실행됨\n",
    "layer = MyLiner(7,12)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .parameter()\n",
    "- 미분 대상이 되는 parameter 값들 확인\n",
    "    - nn.Module 안에 nn.Parameter로 생성한 값들 확인\n",
    "- weight, bias 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3592, -1.0374, -1.7271,  0.6607,  0.3371, -1.5515, -0.4676, -0.6223,\n",
      "          1.9363, -0.8209,  0.3722,  0.1109],\n",
      "        [ 1.5672, -0.4507, -1.2125,  0.3652,  0.0824, -1.4784, -2.4415,  0.0743,\n",
      "          0.1778,  0.0105, -0.5882, -1.0168],\n",
      "        [ 0.3382, -1.0327,  0.6512, -1.1582,  0.2984, -0.4147, -0.0923, -0.6165,\n",
      "         -1.0578,  0.4718,  0.9239,  0.3562],\n",
      "        [-0.5480,  0.5679,  0.1860,  2.2391,  0.5212, -0.8554, -0.1859, -1.2038,\n",
      "          0.4902,  0.5797,  0.5584,  1.2825],\n",
      "        [-0.6254,  0.1903,  0.4651,  1.8050, -0.6152,  0.2653,  0.1783,  0.4419,\n",
      "         -2.1835, -0.6811, -1.1891,  0.2289],\n",
      "        [ 0.5388,  1.5717,  1.2475, -0.7371, -1.7043,  0.4437, -0.2624,  1.1333,\n",
      "         -0.5116, -0.7372, -1.9843,  0.9637],\n",
      "        [ 0.4704, -0.2824, -0.5290, -0.3489, -1.1780,  0.4061, -0.1251,  0.7267,\n",
      "         -1.3462,  0.7394,  1.5415,  0.4353]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.3415,  2.1047,  1.8075, -2.1859, -0.1888,  1.3159,  1.3623, -0.7998,\n",
      "         0.2251, -0.3209,  0.2931,  0.9117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weight and bias\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter 아닌 그냥 Tensor로 wieght 생성했을 때는\n",
    "parameter로 호출 안된다. 그리고 weight가 미분대상으로 지정되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "class MyLiner_tensor(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = Tensor(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = Tensor(torch.randn(out_features))\n",
    "        self.register_buffer('weight',self.weights)\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "\n",
    "layer = MyLiner_tensor(7, 12)\n",
    "x = torch.randn(5,7)\n",
    "layer(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter로 호출 안된다\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register_buffer(name,tensor,persistent=True)\n",
    "- Parameter가 아니어서 자동으로 module에 저장되지는 않지만 따로 저장하고싶은 tensor(module state 등)를 저장\n",
    "- module.name 으로 접근 가능\n",
    "- persistent = True 이면 state_dict()에 저장됨. False면 안저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffer에 추가한 weight가 parameter에는 없지만 layer의 buffer로서 존재<br>state_dict에도 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2949, -0.7042,  1.2433,  0.0889,  0.7479,  0.7894,  0.1380, -0.6044,\n",
       "         -0.7085, -1.3978,  0.2104, -0.0603],\n",
       "        [-0.7519,  0.0176,  2.5754,  1.8879,  0.5698,  0.9581,  0.6077,  0.0663,\n",
       "          1.3105,  1.5825,  0.7167, -0.1105],\n",
       "        [-0.2787, -0.4755, -0.5045, -1.2384,  0.5735, -0.4494,  0.1217, -0.0380,\n",
       "         -1.9169, -1.2761,  0.7776, -0.6836],\n",
       "        [-0.0784, -1.6489,  0.6718, -0.0531, -0.9055, -0.7005,  0.2103, -0.6881,\n",
       "         -0.0070, -1.1194,  0.8467, -0.4971],\n",
       "        [ 0.0262,  0.7233, -2.9718,  0.1400,  0.4020,  1.6114,  2.2128, -0.6321,\n",
       "          0.2116,  0.5547,  0.8738,  0.3467],\n",
       "        [ 0.3371,  0.2767,  1.2433, -0.9884,  0.0419, -0.6650, -1.0619,  0.1339,\n",
       "         -0.4209,  0.0396, -0.7020,  0.7804],\n",
       "        [-0.5884, -1.0025,  0.3156, -0.7788,  0.4656, -1.0590,  1.3068,  0.0381,\n",
       "         -1.0292,  0.5219, -0.0239, -0.5336]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 1.2949, -0.7042,  1.2433,  0.0889,  0.7479,  0.7894,  0.1380, -0.6044,\n",
       "                       -0.7085, -1.3978,  0.2104, -0.0603],\n",
       "                      [-0.7519,  0.0176,  2.5754,  1.8879,  0.5698,  0.9581,  0.6077,  0.0663,\n",
       "                        1.3105,  1.5825,  0.7167, -0.1105],\n",
       "                      [-0.2787, -0.4755, -0.5045, -1.2384,  0.5735, -0.4494,  0.1217, -0.0380,\n",
       "                       -1.9169, -1.2761,  0.7776, -0.6836],\n",
       "                      [-0.0784, -1.6489,  0.6718, -0.0531, -0.9055, -0.7005,  0.2103, -0.6881,\n",
       "                       -0.0070, -1.1194,  0.8467, -0.4971],\n",
       "                      [ 0.0262,  0.7233, -2.9718,  0.1400,  0.4020,  1.6114,  2.2128, -0.6321,\n",
       "                        0.2116,  0.5547,  0.8738,  0.3467],\n",
       "                      [ 0.3371,  0.2767,  1.2433, -0.9884,  0.0419, -0.6650, -1.0619,  0.1339,\n",
       "                       -0.4209,  0.0396, -0.7020,  0.7804],\n",
       "                      [-0.5884, -1.0025,  0.3156, -0.7788,  0.4656, -1.0590,  1.3068,  0.0381,\n",
       "                       -1.0292,  0.5219, -0.0239, -0.5336]]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter 미분\n",
    "- 'Forward의 결과값(예측치,y^)과 실제값 간의 차이', Loss에 대해 미분을 수행 -> Autograd, backward 함수 \n",
    "- Parameter 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch : batch를 한번 돌리는것\n",
    "for epoch in range(epochs):\n",
    "    # 앞 epoch의 grad가 지금 학습에 영향을 주면 안된다. \n",
    "    # gradient 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #y^, 예측치, 결과값\n",
    "    outputs = model(input)\n",
    "\n",
    "    # get loss\n",
    "    #y^: output과 y:lable 간의 loss 값 \n",
    "    loss = creterion(outputs,labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    # weight에 대해 loss를 미분간 값을 보여줌\n",
    "    loss.backward()\n",
    "\n",
    "    #epoch 마다 update\n",
    "    #update parameters\n",
    "    #loss를 줄이는 방향으로 weight 업데이트\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "# ##### For GPU #######\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss값, optimizer 설정\n",
    "- optimize의 대상, learning rate 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 는 mse 채택\n",
    "criterion = torch.nn.MSELoss() \n",
    "# optimizer 는 SGD채택, optimize의 대상은 model.parameters() 입니다. lr 설정.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습\n",
    "1. output (model에 input 넣기)\n",
    "2. loss 값 도출\n",
    "3. 미분(backward)\n",
    "4. step(업데이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    # forward 값 도출\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    # all optimizers implement a step() method, that updates the parameters.\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test : 모델 학습 후 예측값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1247633]\n",
      " [ 3.1067963]\n",
      " [ 5.088829 ]\n",
      " [ 7.070862 ]\n",
      " [ 9.052895 ]\n",
      " [11.034928 ]\n",
      " [13.016961 ]\n",
      " [14.998994 ]\n",
      " [16.981026 ]\n",
      " [18.96306  ]\n",
      " [20.945093 ]]\n"
     ]
    }
   ],
   "source": [
    "# grad 없이 간다.\n",
    "# we don't need gradients in the testing phase\n",
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter 확인(witght, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[1.9820]])\n",
      "None tensor([1.1248])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##logistic regression\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):\n",
    "        super(LR, self).__init__()\n",
    "        # intialize parameters\n",
    "        # 직접 미분을 아래에서 해줄거라 requires_grad = True 해줄 필요 없다.\n",
    "        self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)\n",
    "        self.b = torch.scalar_tensor(0).to(device)\n",
    "        self.grads = {\"dw\": torch.zeros(dim, 1, dtype=torch.float).to(device),\n",
    "                      \"db\": torch.scalar_tensor(0).to(device)}\n",
    "        self.lr = lr.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## compute forward\n",
    "        z = torch.mm(self.w.T, x) + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + torch.exp(-z))\n",
    "\n",
    "    def backward(self, x, yhat, y):\n",
    "        ## compute backward\n",
    "        # loss function에 대해, 각 변수로 미분하고, value를 넣어준 것을 각 grad에 할당\n",
    "        self.grads[\"dw\"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T)\n",
    "        self.grads[\"db\"] = (1/x.shape[1]) * torch.sum(yhat - y)\n",
    "    \n",
    "    def optimize(self):\n",
    "        ## optimization step\n",
    "        # 미분값grad 만큼 업데이트 해줌\n",
    "        self.w = self.w - self.lr * self.grads[\"dw\"]\n",
    "        self.b = self.b - self.lr * self.grads[\"db\"]\n",
    "\n",
    "## utility functions\n",
    "def loss(yhat, y):\n",
    "    m = y.size()[1]\n",
    "    return -(1/m)* torch.sum(y*torch.log(yhat) + (1 - y)* torch.log(1-yhat))\n",
    "\n",
    "def predict(yhat, y):\n",
    "    y_prediction = torch.zeros(1, y.size()[1])\n",
    "    for i in range(yhat.size()[1]):\n",
    "        if yhat[0, i] <= 0.5:\n",
    "            y_prediction[0, i] = 0\n",
    "        else:\n",
    "            y_prediction[0, i] = 1\n",
    "    return 100 - torch.mean(torch.abs(y_prediction - y)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model pretesting\n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "## flatten/transform the data\n",
    "x_flatten = x.T\n",
    "y = y.unsqueeze(0) \n",
    "\n",
    "## num_px is the dimension of the images\n",
    "dim = x_flatten.shape[0]\n",
    "\n",
    "## model instance\n",
    "model = LR(dim)\n",
    "model.to(device)\n",
    "yhat = model.forward(x_flatten.to(device))\n",
    "yhat = yhat.data.cpu()\n",
    "\n",
    "## calculate loss\n",
    "cost = loss(yhat, y)\n",
    "prediction = predict(yhat, y)\n",
    "print(\"Cost: \", cost)\n",
    "print(\"Accuracy: \", prediction)\n",
    "\n",
    "## backpropagate\n",
    "model.backward(x_flatten.to(device), yhat.to(device), y.to(device))\n",
    "model.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
