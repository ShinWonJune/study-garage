{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module\n",
    "- 딥러닝을 구성한든 Layer의 base class\n",
    "- input, output, forward, backward(AutoGrad) 정의\n",
    "- 학습의 대상이 되는 parameter(tensor) 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Parameter\n",
    "- weigh 값을 정의하는 class\n",
    "- Tensor 객체의 상속 객체\n",
    "- nn.Module 내에 attribute가 될 때는 required_grad = True으로 자동 지정되어, AutoGrad의 대상이되어 학습 대상이 되는 Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example ; linear function (xw+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "# layer를 구성하는 base class인 nn.Module을 상속\n",
    "class MyLiner(nn.Module):\n",
    "    # 'in_features'개의 features를 'out_features'개의 feature로.\n",
    "    # bias는 기본적으로 존재한다.\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        # nn.Modeule의 init을 기본적으로 상속\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # weight는 보통 이렇게 까지 직접 지정해주지 않는다.\n",
    "        # weight을 정의하는 nn.Parameter   \n",
    "        # nn.Parameter가 nn.Module 안에 속성이 됐으므로 자동으로 학습대상으로 지정됨(AutoGrad의 대상이 됨). required_grad = True. \n",
    "        self.weights = nn.Parameter(\n",
    "                # input features 개수 X output features의 크기를 갖는 정규분포 난수 weight 생성\n",
    "                torch.randn(in_features,out_features)\n",
    "        )\n",
    "\n",
    "        # Bias 가 True일 때, output feature 개수만큼의 bias 가짐\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    # linear function\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 data, 7 features\n",
    "x = torch.randn(5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1239, -1.2761, -0.3707,  2.0381, -0.0659,  0.3456,  0.2790],\n",
       "        [-0.1320, -1.2445, -0.3816,  0.8433,  0.2786, -0.7823, -0.6969],\n",
       "        [-0.3531,  0.8200, -1.6397,  1.0144,  0.2942,  0.0257, -0.1580],\n",
       "        [ 0.5962,  0.1025,  0.6492,  0.0306, -0.5311,  0.7606, -1.4780],\n",
       "        [ 0.3175, -0.7066, -1.2907, -0.8492,  1.5602,  0.0400,  2.1103]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature를 7에서 12개로 바꿔주고 싶다.\n",
    "layer = MyLiner(7,12)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .parameter()\n",
    "- 미분 대상이 되는 parameter 값들 확인\n",
    "    - nn.Module 안에 nn.Parameter로 생성한 값들 확인\n",
    "- weight, bias 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3592, -1.0374, -1.7271,  0.6607,  0.3371, -1.5515, -0.4676, -0.6223,\n",
      "          1.9363, -0.8209,  0.3722,  0.1109],\n",
      "        [ 1.5672, -0.4507, -1.2125,  0.3652,  0.0824, -1.4784, -2.4415,  0.0743,\n",
      "          0.1778,  0.0105, -0.5882, -1.0168],\n",
      "        [ 0.3382, -1.0327,  0.6512, -1.1582,  0.2984, -0.4147, -0.0923, -0.6165,\n",
      "         -1.0578,  0.4718,  0.9239,  0.3562],\n",
      "        [-0.5480,  0.5679,  0.1860,  2.2391,  0.5212, -0.8554, -0.1859, -1.2038,\n",
      "          0.4902,  0.5797,  0.5584,  1.2825],\n",
      "        [-0.6254,  0.1903,  0.4651,  1.8050, -0.6152,  0.2653,  0.1783,  0.4419,\n",
      "         -2.1835, -0.6811, -1.1891,  0.2289],\n",
      "        [ 0.5388,  1.5717,  1.2475, -0.7371, -1.7043,  0.4437, -0.2624,  1.1333,\n",
      "         -0.5116, -0.7372, -1.9843,  0.9637],\n",
      "        [ 0.4704, -0.2824, -0.5290, -0.3489, -1.1780,  0.4061, -0.1251,  0.7267,\n",
      "         -1.3462,  0.7394,  1.5415,  0.4353]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.3415,  2.1047,  1.8075, -2.1859, -0.1888,  1.3159,  1.3623, -0.7998,\n",
      "         0.2251, -0.3209,  0.2931,  0.9117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weight and bias\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter 아닌 그냥 Tensor로 wieght 생성했을 때는\n",
    "parameter로 호출 안된다. 그리고 weight가 미분대상으로 지정되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class MyLiner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = Tensor(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = Tensor(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "\n",
    "layer = MyLiner(7, 12)\n",
    "layer(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter로 호출 안된다\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter 미분\n",
    "- 'Forward의 결과값(예측치,y^)과 실제값 간의 차이', Loss에 대해 미분을 수행 -> Autograd, backward 함수 \n",
    "- Parameter 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch : batch를 한번 돌리는것\n",
    "for epoch in range(epochs):\n",
    "    # 앞 epoch의 grad가 지금 학습에 영향을 주면 안된다. \n",
    "    # gradient 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #y^, 예측치, 결과값\n",
    "    outputs = model(input)\n",
    "\n",
    "    # get loss\n",
    "    #y^: output과 y:lable 간의 loss 값 \n",
    "    loss = creterion(outputs,labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    # weight에 대해 loss를 미분간 값을 보여줌\n",
    "    loss.backward()\n",
    "\n",
    "    #epoch 마다 update\n",
    "    #update parameters\n",
    "    #loss를 줄이는 방향으로 weight 업데이트\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "# ##### For GPU #######\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss값, optimizer 설정\n",
    "- optimize의 대상, learning rate 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 는 mse 채택\n",
    "criterion = torch.nn.MSELoss() \n",
    "# optimizer 는 SGD채택, optimize의 대상은 model.parameters() 입니다. lr 설정.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습\n",
    "1. output (model에 input 넣기)\n",
    "2. loss 값 도출\n",
    "3. 미분(backward)\n",
    "4. step(업데이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    # forward 값 도출\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    # all optimizers implement a step() method, that updates the parameters.\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test : 모델 학습 후 예측값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1247633]\n",
      " [ 3.1067963]\n",
      " [ 5.088829 ]\n",
      " [ 7.070862 ]\n",
      " [ 9.052895 ]\n",
      " [11.034928 ]\n",
      " [13.016961 ]\n",
      " [14.998994 ]\n",
      " [16.981026 ]\n",
      " [18.96306  ]\n",
      " [20.945093 ]]\n"
     ]
    }
   ],
   "source": [
    "# grad 없이 간다.\n",
    "# we don't need gradients in the testing phase\n",
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter 확인(witght, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[1.9820]])\n",
      "None tensor([1.1248])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##logistic regression\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):\n",
    "        super(LR, self).__init__()\n",
    "        # intialize parameters\n",
    "        # 직접 미분을 아래에서 해줄거라 requires_grad = True 해줄 필요 없다.\n",
    "        self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)\n",
    "        self.b = torch.scalar_tensor(0).to(device)\n",
    "        self.grads = {\"dw\": torch.zeros(dim, 1, dtype=torch.float).to(device),\n",
    "                      \"db\": torch.scalar_tensor(0).to(device)}\n",
    "        self.lr = lr.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## compute forward\n",
    "        z = torch.mm(self.w.T, x) + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + torch.exp(-z))\n",
    "\n",
    "    def backward(self, x, yhat, y):\n",
    "        ## compute backward\n",
    "        # loss function에 대해, 각 변수로 미분하고, value를 넣어준 것을 각 grad에 할당\n",
    "        self.grads[\"dw\"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T)\n",
    "        self.grads[\"db\"] = (1/x.shape[1]) * torch.sum(yhat - y)\n",
    "    \n",
    "    def optimize(self):\n",
    "        ## optimization step\n",
    "        # 미분값grad 만큼 업데이트 해줌\n",
    "        self.w = self.w - self.lr * self.grads[\"dw\"]\n",
    "        self.b = self.b - self.lr * self.grads[\"db\"]\n",
    "\n",
    "## utility functions\n",
    "def loss(yhat, y):\n",
    "    m = y.size()[1]\n",
    "    return -(1/m)* torch.sum(y*torch.log(yhat) + (1 - y)* torch.log(1-yhat))\n",
    "\n",
    "def predict(yhat, y):\n",
    "    y_prediction = torch.zeros(1, y.size()[1])\n",
    "    for i in range(yhat.size()[1]):\n",
    "        if yhat[0, i] <= 0.5:\n",
    "            y_prediction[0, i] = 0\n",
    "        else:\n",
    "            y_prediction[0, i] = 1\n",
    "    return 100 - torch.mean(torch.abs(y_prediction - y)) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
