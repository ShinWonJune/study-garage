{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module\n",
    "- 딥러닝을 구성한든 Layer의 base class\n",
    "- input, output, forward, backward(AutoGrad) 정의\n",
    "- 학습의 대상이 되는 parameter(tensor) 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Parameter\n",
    "- weigh 값을 정의하는 class\n",
    "- Tensor 객체의 상속 객체\n",
    "- nn.Module 내에 attribute가 될 때는 required_grad = True으로 자동 지정되어, AutoGrad의 대상이되어 학습 대상이 되는 Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example ; linear function (xw+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "# layer를 구성하는 base class인 nn.Module을 상속\n",
    "class MyLiner(nn.Module):\n",
    "    # 'in_features'개의 features를 'out_features'개의 feature로.\n",
    "    # bias는 기본적으로 존재한다.\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        # nn.Modeule의 init을 기본적으로 상속\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # weight는 보통 이렇게 까지 직접 지정해주지 않는다.\n",
    "        # weight을 정의하는 nn.Parameter   \n",
    "        # nn.Parameter가 nn.Module 안에 속성이 됐으므로 자동으로 학습대상으로 지정됨(AutoGrad의 대상이 됨). required_grad = True. \n",
    "        self.weights = nn.Parameter(\n",
    "                # input features 개수 X output features의 크기를 갖는 정규분포 난수 weight 생성\n",
    "                torch.randn(in_features,out_features)\n",
    "        )\n",
    "\n",
    "        # Bias 가 True일 때, output feature 개수만큼의 bias 가짐\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    # linear function\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 data, 7 features\n",
    "x = torch.randn(5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1239, -1.2761, -0.3707,  2.0381, -0.0659,  0.3456,  0.2790],\n",
       "        [-0.1320, -1.2445, -0.3816,  0.8433,  0.2786, -0.7823, -0.6969],\n",
       "        [-0.3531,  0.8200, -1.6397,  1.0144,  0.2942,  0.0257, -0.1580],\n",
       "        [ 0.5962,  0.1025,  0.6492,  0.0306, -0.5311,  0.7606, -1.4780],\n",
       "        [ 0.3175, -0.7066, -1.2907, -0.8492,  1.5602,  0.0400,  2.1103]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature를 7에서 12개로 바꿔주고 싶다.\n",
    "layer = MyLiner(7,12)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .parameter()\n",
    "- 미분 대상이 되는 parameter 값들 확인\n",
    "    - nn.Module 안에 nn.Parameter로 생성한 값들 확인\n",
    "- weight, bias 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3592, -1.0374, -1.7271,  0.6607,  0.3371, -1.5515, -0.4676, -0.6223,\n",
      "          1.9363, -0.8209,  0.3722,  0.1109],\n",
      "        [ 1.5672, -0.4507, -1.2125,  0.3652,  0.0824, -1.4784, -2.4415,  0.0743,\n",
      "          0.1778,  0.0105, -0.5882, -1.0168],\n",
      "        [ 0.3382, -1.0327,  0.6512, -1.1582,  0.2984, -0.4147, -0.0923, -0.6165,\n",
      "         -1.0578,  0.4718,  0.9239,  0.3562],\n",
      "        [-0.5480,  0.5679,  0.1860,  2.2391,  0.5212, -0.8554, -0.1859, -1.2038,\n",
      "          0.4902,  0.5797,  0.5584,  1.2825],\n",
      "        [-0.6254,  0.1903,  0.4651,  1.8050, -0.6152,  0.2653,  0.1783,  0.4419,\n",
      "         -2.1835, -0.6811, -1.1891,  0.2289],\n",
      "        [ 0.5388,  1.5717,  1.2475, -0.7371, -1.7043,  0.4437, -0.2624,  1.1333,\n",
      "         -0.5116, -0.7372, -1.9843,  0.9637],\n",
      "        [ 0.4704, -0.2824, -0.5290, -0.3489, -1.1780,  0.4061, -0.1251,  0.7267,\n",
      "         -1.3462,  0.7394,  1.5415,  0.4353]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.3415,  2.1047,  1.8075, -2.1859, -0.1888,  1.3159,  1.3623, -0.7998,\n",
      "         0.2251, -0.3209,  0.2931,  0.9117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# weight and bias\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter 아닌 그냥 Tensor로 wieght 생성했을 때는\n",
    "parameter로 호출 안된다. 그리고 weight가 미분대상으로 지정되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class MyLiner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = Tensor(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = Tensor(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "\n",
    "layer = MyLiner(7, 12)\n",
    "layer(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter로 호출 안된다\n",
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter 미분\n",
    "- 'Forward의 결과값(예측치,y^)과 실제값 간의 차이', Loss에 대해 미분을 수행 -> Autograd, backward 함수 \n",
    "- Parameter 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch : batch를 한번 돌리는것\n",
    "for epoch in range(epochs):\n",
    "    # 앞 epoch의 grad가 지금 학습에 영향을 주면 안된다. \n",
    "    # gradient 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #y^, 예측치, 결과값\n",
    "    outputs = model(input)\n",
    "\n",
    "    # get loss\n",
    "    #y^: output과 y:lable 간의 loss 값 \n",
    "    loss = creterion(outputs,labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    # weight에 대해 loss를 미분간 값을 보여줌\n",
    "    loss.backward()\n",
    "\n",
    "    #epoch 마다 update\n",
    "    #update parameters\n",
    "    #loss를 줄이는 방향으로 weight 업데이트\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "# ##### For GPU #######\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss값, optimizer 설정\n",
    "- optimize의 대상, learning rate 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 는 mse 채택\n",
    "criterion = torch.nn.MSELoss() \n",
    "# optimizer 는 SGD채택, optimize의 대상은 model.parameters() 입니다. lr 설정.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습\n",
    "1. output (model에 input 넣기)\n",
    "2. loss 값 도출\n",
    "3. 미분(backward)\n",
    "4. step(업데이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91.6573, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 91.65726470947266\n",
      "tensor(7.4886, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 7.488600254058838\n",
      "tensor(0.6231, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 0.6230887770652771\n",
      "tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 0.06295282393693924\n",
      "tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 0.017129166051745415\n",
      "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 0.01325754914432764\n",
      "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 0.012809330597519875\n",
      "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 0.012641806155443192\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 0.012498593889176846\n",
      "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 0.012358838692307472\n",
      "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "epoch 10, loss 0.01222088560461998\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "epoch 11, loss 0.01208441611379385\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "epoch 12, loss 0.011949458159506321\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "epoch 13, loss 0.011815976351499557\n",
      "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "epoch 14, loss 0.01168403122574091\n",
      "tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "epoch 15, loss 0.011553600430488586\n",
      "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "epoch 16, loss 0.011424589902162552\n",
      "tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "epoch 17, loss 0.011297020129859447\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "epoch 18, loss 0.011170870624482632\n",
      "tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "epoch 19, loss 0.011046125553548336\n",
      "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "epoch 20, loss 0.010922739282250404\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "epoch 21, loss 0.01080077700316906\n",
      "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "epoch 22, loss 0.010680175386369228\n",
      "tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "epoch 23, loss 0.01056094840168953\n",
      "tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "epoch 24, loss 0.010442993603646755\n",
      "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "epoch 25, loss 0.01032637245953083\n",
      "tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "epoch 26, loss 0.010211055167019367\n",
      "tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "epoch 27, loss 0.010097033344209194\n",
      "tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "epoch 28, loss 0.009984266012907028\n",
      "tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "epoch 29, loss 0.00987279787659645\n",
      "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "epoch 30, loss 0.009762539528310299\n",
      "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "epoch 31, loss 0.009653558023273945\n",
      "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "epoch 32, loss 0.009545727632939816\n",
      "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "epoch 33, loss 0.009439129382371902\n",
      "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "epoch 34, loss 0.009333726018667221\n",
      "tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "epoch 35, loss 0.009229511953890324\n",
      "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "epoch 36, loss 0.009126401506364346\n",
      "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "epoch 37, loss 0.009024531580507755\n",
      "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "epoch 38, loss 0.008923756889998913\n",
      "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "epoch 39, loss 0.008824062533676624\n",
      "tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "epoch 40, loss 0.008725532330572605\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "epoch 41, loss 0.008628119714558125\n",
      "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "epoch 42, loss 0.008531755767762661\n",
      "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "epoch 43, loss 0.00843653455376625\n",
      "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "epoch 44, loss 0.008342292159795761\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "epoch 45, loss 0.008249098435044289\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "epoch 46, loss 0.008157008327543736\n",
      "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "epoch 47, loss 0.008065950125455856\n",
      "tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "epoch 48, loss 0.007975857704877853\n",
      "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "epoch 49, loss 0.007886798121035099\n",
      "tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "epoch 50, loss 0.007798738777637482\n",
      "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "epoch 51, loss 0.007711606100201607\n",
      "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "epoch 52, loss 0.00762554444372654\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "epoch 53, loss 0.007540352642536163\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "epoch 54, loss 0.007456172723323107\n",
      "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "epoch 55, loss 0.007372887805104256\n",
      "tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "epoch 56, loss 0.007290592882782221\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "epoch 57, loss 0.007209185045212507\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "epoch 58, loss 0.007128673605620861\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "epoch 59, loss 0.007049072068184614\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "epoch 60, loss 0.006970346439629793\n",
      "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "epoch 61, loss 0.006892489269375801\n",
      "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "epoch 62, loss 0.0068155559711158276\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "epoch 63, loss 0.006739396136254072\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "epoch 64, loss 0.006664178799837828\n",
      "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "epoch 65, loss 0.006589756812900305\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "epoch 66, loss 0.006516155321151018\n",
      "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "epoch 67, loss 0.006443435326218605\n",
      "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "epoch 68, loss 0.006371424999088049\n",
      "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "epoch 69, loss 0.006300330627709627\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "epoch 70, loss 0.006229966413229704\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "epoch 71, loss 0.006160371471196413\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "epoch 72, loss 0.006091596558690071\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "epoch 73, loss 0.006023590452969074\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "epoch 74, loss 0.005956295412033796\n",
      "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "epoch 75, loss 0.005889791529625654\n",
      "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "epoch 76, loss 0.005824030842632055\n",
      "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "epoch 77, loss 0.005758975632488728\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "epoch 78, loss 0.005694675259292126\n",
      "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "epoch 79, loss 0.005631087813526392\n",
      "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "epoch 80, loss 0.005568185355514288\n",
      "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "epoch 81, loss 0.0055060307495296\n",
      "tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "epoch 82, loss 0.0054445513524115086\n",
      "tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "epoch 83, loss 0.005383746232837439\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "epoch 84, loss 0.005323631223291159\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "epoch 85, loss 0.005264148116111755\n",
      "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "epoch 86, loss 0.005205381195992231\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "epoch 87, loss 0.005147253628820181\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "epoch 88, loss 0.0050897602923214436\n",
      "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "epoch 89, loss 0.005032964050769806\n",
      "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "epoch 90, loss 0.004976725205779076\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "epoch 91, loss 0.004921195562928915\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "epoch 92, loss 0.0048662214539945126\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "epoch 93, loss 0.004811886232346296\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "epoch 94, loss 0.004758140072226524\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "epoch 95, loss 0.004705026280134916\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "epoch 96, loss 0.0046524470672011375\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "epoch 97, loss 0.004600491840392351\n",
      "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "epoch 98, loss 0.004549133125692606\n",
      "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "epoch 99, loss 0.004498339723795652\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    # all optimizers implement a step() method, that updates the parameters.\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION<br>\n",
    "reevaluating the function multiple times 가 필요할 때 아래와 같은 방식을 택한다고 하는데 어디서 recompute가 일어남?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test : 모델 학습 후 예측값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1247633]\n",
      " [ 3.1067963]\n",
      " [ 5.088829 ]\n",
      " [ 7.070862 ]\n",
      " [ 9.052895 ]\n",
      " [11.034928 ]\n",
      " [13.016961 ]\n",
      " [14.998994 ]\n",
      " [16.981026 ]\n",
      " [18.96306  ]\n",
      " [20.945093 ]]\n"
     ]
    }
   ],
   "source": [
    "# grad 없이 간다.\n",
    "# we don't need gradients in the testing phase\n",
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter 확인(witght, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[1.9820]])\n",
      "None tensor([1.1248])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.name, p.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
