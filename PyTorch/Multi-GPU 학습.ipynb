{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개념정리\n",
    "- single vs Multi\n",
    "- GPU VS Node(computer 1대)\n",
    "- Single Node Multi GPU : 한대의 컴퓨터에 여러 gpu가 설치됨\n",
    "- Multi Node  Mulit GPU : 여러 컴퓨터 여러 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parallel\n",
    "- 모델을 다중 GPU에 분산하여 학습하는것\n",
    "- 모델 병렬화\n",
    "- alexnet\n",
    "- 병목, 파이프라인 등의 어려움으로 고난이도임\n",
    "- .to(gpu) 해서 보내줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallel\n",
    "- 데이터를 다중 GPU 에 분산하여 학습\n",
    "- 각각의 LOSS값을 통해 역전파를 한뒤 gradient의 평균을 구하여 weight update를 해줌\n",
    "- loss값 도출과 grad평균으로 weight update는 하나의 gpu에서 함\n",
    "- mini batch 수식과 유사하나 한번에 여러 gpu에서 수행함\n",
    "\n",
    "    #### 두가지 방식\n",
    "    - DataParallel\n",
    "        - 단순히 데이터 분배 후 평균\n",
    "        - GPU 사용 불균형 문제 발생\n",
    "        - 중앙 coordinate GPU에 병목현상 발생        \n",
    "        - __이를 고려하여 batch size 를 감소시켜줘야함__\n",
    "        - GIL 문제\n",
    "    - DistributedDataParallel\n",
    "        - 각각 cpu를 할당하여 loss, grad를 구한뒤 평균을 구함\n",
    "        - 하나의 gpu가 coordinate하지 않음\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paralle_model = torch.nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistributedDataParallel\n",
    "- sampler를 만들어줘야함\n",
    "- shuffle = False\n",
    "- pin_memory = True\n",
    "- num_workers : gpu의 갯수 곱하기 4 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "shuffle = False\n",
    "# 메모리에 바로바로 올릴 수 있도록 절차를 간소하게 하는 것.\n",
    "pin_memory = True\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size = 20, shuffle = False, \n",
    "            pin_memory = pin_memory, num_workers = 4, \n",
    "            shuffle = shuffle, sampler = train_sampler)\n",
    "# 알아서 gpu에 각각 데이터를 넣어줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    torch.multiprocessing.spawn(main_worker, nprocs = n_gpus, args = (n_gpus, ))\n",
    "    # n_gpus : gpu수 * 4\n",
    "def main_worker(gpu, n_gpus):\n",
    "\n",
    "    image_size = 224\n",
    "    batch_size = 512\n",
    "    num_worker = 8\n",
    "    epochs = ...\n",
    "\n",
    "    batch_size = int(batch_size / n_gpus)\n",
    "    num_worker = int(num_worker / n_gpus)\n",
    "\n",
    "    torch.distributed.init_process_group(\n",
    "                                            # gpu 끼리 통신하는 멀티프로세싱 통신 규약 정의\n",
    "            backend = 'nccl', init_metod = 'tcp://127.0.0.1:2569 ', world_zier = n_gpus, rank = gpu)\n",
    "    )\n",
    "\n",
    "    model = MODEL\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model = model.cuda(gpu)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, divice_ids=[gpu])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
