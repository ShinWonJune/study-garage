{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.index_select(input,dim,index) -> Tensor\n",
    "- returns new tensor indexes the input tensor along dimension dim using entries in index, which is a longtensor\n",
    "- 인덱스와 axis로 tensor value 선택하여 새로운 tensor 반환\n",
    "- index : 1-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shinw\\OneDrive\\문서\\GitHub\\study-garage\\PyTorch\\Often used torch.ipynb 셀 2\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tensor([[ \u001b[39m0.1427\u001b[39m,  \u001b[39m0.0231\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.5414\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.0009\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         [\u001b[39m-\u001b[39m\u001b[39m0.4664\u001b[39m,  \u001b[39m0.2647\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.1228\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.1068\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         [\u001b[39m-\u001b[39m\u001b[39m1.1734\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.6571\u001b[39m,  \u001b[39m0.7230\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.6004\u001b[39m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/Often%20used%20torch.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m torch\u001b[39m.\u001b[39mindex_select(x, \u001b[39m0\u001b[39m, indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    ">>> x = torch.randn(3, 4)\n",
    ">>> x\n",
    "tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
    "        [-0.4664,  0.2647, -0.1228, -1.1068],\n",
    "        [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
    ">>> indices = torch.tensor([0, 2])\n",
    ">>> torch.index_select(x, 0, indices)\n",
    "tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
    "        [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
    ">>> torch.index_select(x, 1, indices)\n",
    "tensor([[ 0.1427, -0.5414],\n",
    "        [-0.4664, -0.1228],\n",
    "        [-1.1734,  0.7230]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.gather(input, dim, index)\n",
    "- dim(axis)과 index 기준으로 input의 value 가져와서 새로운 tensor 생성\n",
    "- index의 dim이 input의 dim과 같거나 작다. 즉 index_select 보다 더 입체적으로 value를 가져올 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0<br>\n",
    "out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1<br>\n",
    "out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "indices0 = torch.tensor([[0,1],[1,1]])\n",
    "indices1 = torch.tensor([[1,0],[0,1]])\n",
    "t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim = 1\n",
    "- input index 둘 다 axis = 1 방향으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t, 1, torch.tensor([[0,1],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t, 1, torch.tensor([[1,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim = 0\n",
    "- 혼란함\n",
    "- input, index 둘 다 axis 0 방향으로 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [1, 4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t,0,torch.tensor([[0,1],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t, 0, torch.tensor([[1,1],[0,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.scatter_(dim,src,index)\n",
    "- index의 value를 target tensor의 index 삼아 src의 값을 넣어준다\n",
    "    - dim의 방향대로 target과 src에 접근한다\n",
    "    - target tensor의 index 생성 : <br>\n",
    "   target tensor index의 dimension 'dim'에 index의 value, 이외 dimension에는 index value의 해당 dimension 위치 값을 할당하여 target tensor의 index를 생성한다.\n",
    "        - dim = 0 , index = [[a,b]] then target[a,__0__], target[b,__1__], where index value a's dimension 위치 값 = (0,__0__) and index value b's dimension 위치 값 = (0,__1__)\n",
    "        - dim = 1 , index = [[a],[b]] then target[0,a], target[1,b], where index value a's dimension 위치 값 = (__0__,0) and index value b's dimension 위치 값 = (__1__,0)\n",
    "    - 생성된 target index에 src의 value를 넣는다. 이때 scatter 할 src의 value 또한 dim 방향으로 접근한다.\n",
    "        - if src = [[1,2],[3,4]], and dim = 0 , then 1, 3, 2, 4 순서대로 target에 넣는다\n",
    "    - self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0<br>\n",
    "self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1<br>\n",
    "self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
    "                    \n",
    "- gather와 반대방향의 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> src = torch.arange(1, 11).reshape((2, 5))\n",
    ">>> src\n",
    "tensor([[ 1,  2,  3,  4,  5],\n",
    "        [ 6,  7,  8,  9, 10]])\n",
    ">>> index = torch.tensor([[0, 1, 2, 0]])\n",
    ">>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
    "tensor([[1, 0, 0, 4, 0],\n",
    "        [0, 2, 0, 0, 0],\n",
    "        [0, 0, 3, 0, 0]])\n",
    ">>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
    ">>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
    "tensor([[1, 2, 3, 0, 0],\n",
    "        [6, 7, 0, 0, 8],\n",
    "        [0, 0, 0, 0, 0]])\n",
    "\n",
    ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
    "...            1.23, reduce='multiply')\n",
    "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
    "        [2.0000, 2.0000, 2.0000, 2.4600]])\n",
    ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
    "...            1.23, reduce='add')\n",
    "tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
    "        [2.0000, 2.0000, 2.0000, 3.2300]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "옵션 reduce='add',or'multipy'\n",
    "- target tensor에 src값을 더거나 곱하여 값을 누적시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
    "...            1.23, reduce='multiply')\n",
    "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
    "        [2.0000, 2.0000, 2.0000, 2.4600]])\n",
    ">>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
    "...            1.23, reduce='add')\n",
    "tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
    "        [2.0000, 2.0000, 2.0000, 3.2300]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
