{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy + AutoGrad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor\n",
    "- 다차원 Array를 표현하는 PyTorch 클래스\n",
    "- Numpy의 ndarray와 동일\n",
    "- Tensor 생성 함수도 거의 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.tensor(obj)\n",
    "#### torch.FloatTensor(obj)\n",
    "- array, ndarray, list를 Float형 tensor로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8., 9.]])\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_array = np.arange(10).reshape(2,5) \n",
    "\n",
    "#Float형 Tensor로 만드는 함수. aFloatTensor\n",
    "t_array = torch.FloatTensor(n_array)\n",
    "t_array2 = torch.FloatTensor([1,2,3,4])\n",
    "print(t_array)\n",
    "print(t_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.from_numpy(ndarray)\n",
    "- ndarray -> tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy의 사용법이 그대로 적용됨\n",
    "- slicing\n",
    "- flatten()\n",
    "- ones_like(object)\n",
    "    - object와 같은 크기의 1로 이루어진 객체 생성\n",
    "- shape\n",
    "- dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.numpy()\n",
    "   - tensor -> ndarray (numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linspace(start,end,n)\n",
    "- 범위 내 균등한 간격의 n개의 값을 갖는 tensor 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 2.3333, 3.6667, 5.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1,5,4) #4개 균일한 간격으로 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or CPU\n",
    "- x_data.device\n",
    "    - if cpu -> 'cpu'\n",
    "    - if gpu -> 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_data_cuda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shinw\\OneDrive\\문서\\GitHub\\study-garage\\PyTorch\\PyTorch Basics.ipynb 셀 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     x_data_cuda \u001b[39m=\u001b[39m x_data\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x_data_cuda\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_data_cuda' is not defined"
     ]
    }
   ],
   "source": [
    "from zoneinfo import available_timezones\n",
    "\n",
    "#in colab\n",
    "if torch.cuda.is_available():\n",
    "    x_data_cuda = x_data.to('cuda')\n",
    "# x_data_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view\n",
    "- numpy 의 reshape과 거의 동일\n",
    "- view는 reshape된 형태만 보여줌 새로운 메모리에 저장하지 않음\n",
    "- reshape은 메모리 구조가 깨지면 새로운 객체에 copy해서 보여줌\n",
    "- view를 쓰면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8573, 0.1220],\n",
       "         [0.7868, 0.2929],\n",
       "         [0.9471, 0.8193]],\n",
       "\n",
       "        [[0.8964, 0.9736],\n",
       "         [0.8400, 0.7318],\n",
       "         [0.6424, 0.6319]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_ex = torch.rand(size=(2,3,2))\n",
    "tensor_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8573, 0.1220, 0.7868, 0.2929, 0.9471, 0.8193],\n",
       "        [0.8964, 0.9736, 0.8400, 0.7318, 0.6424, 0.6319]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view, 형태를 바꿔서 보여줌\n",
    "tensor_ex.view([-1,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 따로 할당하지 않음\n",
    "a = torch.zeros(2,3)\n",
    "b = a.view([-1,6])\n",
    "a.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a에 1을 채웠는데 b도 1이 찼다. copy가 아님. 같은 메모리를 씀. \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.zeros(3,2)\n",
    "b=a.t().reshape(2,3)\n",
    "c = a.t().reshape([-1,6])\n",
    "a.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.t()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape도 차원이 깨지지 않는이상 같은 메모리를 보여준다\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 구조가 깨지면 copy해서 새로운 객체로 보여준다. \n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor.clone().detach()\n",
    "- Deepcopy\n",
    "- .clone(): 내용을 복사한 텐서 생성(그러나 기존 tensor의 training graph 메모리에 존재)\n",
    "- .detach() : Deeplearning training graph에서 벗어나 아예 따로 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 타입 변환 type\n",
    "- Tensor = torch.type(Datatype)\n",
    "datatype으로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float64 torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data1 = torch.zeros(2,dtype=torch.float) #32float\n",
    "data2 = torch.ones(2,4, dtype=torch.double)\n",
    "data3 = torch.rand(2,2,3,dtype=torch.half)\n",
    "print(data1.dtype,data2.dtype,data3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int32 torch.float64\n"
     ]
    }
   ],
   "source": [
    "data1 = data1.type(torch.float32)\n",
    "data2 = data2.type(torch.int)\n",
    "data3 = data3.type(torch.double)\n",
    "print(data1.dtype,data2.dtype,data3.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size\n",
    "- Tensor 의 axis별 크기 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1's size: torch.Size([2, 2, 3]) \n",
      " axix 0 size: 2 \n",
      " axis 1 size: 2 \n",
      " axis 2 size: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data1= torch.DoubleTensor([\n",
    "    [[1,2,3],\n",
    "     [4,5,6]],\n",
    "    [[7,8,9],\n",
    "    [10,11,12]]\n",
    "])\n",
    "print(\"data1's size:\",data1.shape,'\\n','axix 0 size:',data1.size(0),'\\n','axis 1 size:',data1.size(1),'\\n','axis 2 size:',data1.size(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze\n",
    "- 차원의 개수가 1인 차원을 삭제 (압축)\n",
    "\n",
    "### tensor.unsqueeze(axis) or torch.unsqueeze(tensor,axis)\n",
    "-  axis 방향으로 dim을 지정해서 해당 dim에 차원을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7636, 0.6494],\n",
      "        [0.3334, 0.1886]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor_ex = torch.rand(size=(2,1,2))\n",
    "tensor_ex = tensor_ex.squeeze()\n",
    "print(tensor_ex, tensor_ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2]) \n",
      " tensor([[[0.7636, 0.6494],\n",
      "         [0.3334, 0.1886]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_ex.unsqueeze(0).shape\n",
    "print(tensor_ex.unsqueeze(0).shape,'\\n', tensor_ex.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2]) \n",
      " tensor([[[0.7636, 0.6494]],\n",
      "\n",
      "        [[0.3334, 0.1886]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_ex.unsqueeze(1).shape\n",
    "print(tensor_ex.unsqueeze(1).shape,'\\n', tensor_ex.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1]) \n",
      " tensor([[[0.7636],\n",
      "         [0.6494]],\n",
      "\n",
      "        [[0.3334],\n",
      "         [0.1886]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_ex.unsqueeze(2).shape\n",
    "print(tensor_ex.unsqueeze(2).shape,'\\n', tensor_ex.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operations\n",
    "- numpy와 거의 유사\n",
    "- 차원이 다르면 개진다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬곱이란? \n",
    "- 수학시간의 행렬 곱\n",
    "- vector 차원에서는 내적\n",
    "- Not element wise operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mm\n",
    "- 행렬곱셈.(scalar, vector는 안됨)\n",
    "- numpy는 dot이지만 Torch는 mm\n",
    "- dot은 scalar나 vector일때만."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60.,  70.],\n",
       "        [160., 195.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2 = np.arange(10).reshape(5,2)\n",
    "n1 = np.arange(10).reshape(2,5)\n",
    "t1 = torch.FloatTensor(n1)\n",
    "t2 = torch.FloatTensor(n2)\n",
    "t1.mm(t2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60.,  70.],\n",
       "        [160., 195.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.matmul(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dot\n",
    "- scalar, vector 내적 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(285.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = t1.reshape(10)\n",
    "t2 = t2.reshape(10)\n",
    "t1.dot(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matmul\n",
    "- shape이 맞을 때는 행렬내적\n",
    "- shape이 안맞을 때는 broadcastind이 일어남\n",
    "- ex. shape(5,2,3) matmul shape(3)\n",
    "    - dim 0는 batch로 인식\n",
    "    - batch 5 인 (2,3) matmul (3,1) 로 연산이 됨\n",
    "    - 5개의 layer 마다 (2,3) mm (3,1) 하여 (5,2,1) shape의 값이 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shinw\\OneDrive\\문서\\GitHub\\study-garage\\PyTorch\\PyTorch Basics.ipynb 셀 31\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m5\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shinw/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/study-garage/PyTorch/PyTorch%20Basics.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a\u001b[39m.\u001b[39;49mmm(b)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "# shape이 안맞아서 연산 불가\n",
    "a = torch.rand(5,2,3)\n",
    "b = torch.rand(3) # 크기 3의 벡터\n",
    "a.mm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1071, 0.6791],\n",
       "        [0.3910, 0.3060],\n",
       "        [0.4981, 0.5321],\n",
       "        [0.4815, 0.2422],\n",
       "        [0.6298, 0.3041]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 batch의 (2,3) 과 (3,1)의 mm으로 연산\n",
    "a = torch.rand(5,2,3)\n",
    "b = torch.rand(3)\n",
    "a.matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1071],\n",
      "        [0.6791]])\n",
      "tensor([[0.3910],\n",
      "        [0.3060]])\n",
      "tensor([[0.4981],\n",
      "        [0.5321]])\n",
      "tensor([[0.4815],\n",
      "        [0.2422]])\n",
      "tensor([[0.6298],\n",
      "        [0.3041]])\n"
     ]
    }
   ],
   "source": [
    "# mm으로 표현하면 아래와 같다.\n",
    "print(a[0].mm(torch.unsqueeze(b,1)))\n",
    "print(a[1].mm(torch.unsqueeze(b,1)))\n",
    "print(a[2].mm(torch.unsqueeze(b,1)))\n",
    "print(a[3].mm(torch.unsqueeze(b,1)))\n",
    "print(a[4].mm(torch.unsqueeze(b,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadCasting\n",
    "- 사칙연산 중, Tensor 차원이 다를 경우 발생\n",
    "- 차원이 달라도 각 차원의 원소 수는 같아야함\n",
    "    - ex. (10,3,5) * (3,5)\n",
    "- 서로 다른 차원의 원소가 없거나 1일 경우\n",
    "    - (10,1,5) * (10,3,1)\n",
    "    - (10,3,5) * (3,1)\n",
    "    - (10,3,5) * (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 통계값\n",
    "- .min(), .max() : argment return\n",
    "- .argmin(), .argmax() : index return\n",
    "    - tensor를 1차원으로 풀었을 때의 index를 반환. \n",
    "    - tensor(n) 과 같이 tensor(int) 형태로 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor save\n",
    " - torch.save(Tensor,'filename.pt')\n",
    " - Dict 형태로 여러개의 tensor 한번에 저장가능\n",
    "    - 불러 올 때도 여러개 불러옴. 각 tensor의 이름으로 호출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.linspace(1,5,4)\n",
    "data2 = torch.linspace(1,10,4)\n",
    "data3 = torch.linspace(1,100,4)\n",
    "datas = {'data1':data1,'data2':data2,'data3':data3}\n",
    "torch.save(datas,'mydata1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 2.3333, 3.6667, 5.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = torch.load('mydata1.pt')\n",
    "datas['data1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.functional 모듈\n",
    "- DL함수를 통해 다양한 수식 변환 지원\n",
    "- nn.functional.softmax\n",
    "- nn.functional.argmax -> nn.functional.one_hot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one hot incoding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 4, 4],\n",
       "        [2, 2, 2, 2, 0],\n",
       "        [3, 0, 2, 0, 4],\n",
       "        [3, 4, 2, 4, 3],\n",
       "        [1, 0, 3, 2, 2],\n",
       "        [1, 4, 1, 0, 1],\n",
       "        [2, 0, 1, 2, 3],\n",
       "        [2, 0, 2, 3, 0],\n",
       "        [3, 3, 0, 2, 3],\n",
       "        [0, 1, 3, 4, 2]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n1 = torch.rand(3)\n",
    "tensor  = torch.FloatTensor(n1)\n",
    "h_tensor = F.softmax(tensor,dim = 0)\n",
    "h_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 2, 1, 2, 0, 4, 0, 1, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0~5 사이의 random int in shape of 10x5\n",
    "y = torch.randint(5,(10,5))\n",
    "# max arg's indexes in dim 1\n",
    "y_lable = y.argmax(dim=1)\n",
    "y_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot 인코딩. target lable만 1 나머지는 0\n",
    "# y 의 shape 정보가 남아있어서 10 by 5 형태로 출력해줌\n",
    "F.one_hot(y_lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad\n",
    "- 자동 미분지원\n",
    "- backward 함수 사용\n",
    "    - graph leaves에 대한 gradient를 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 미분할 변수 대상에 requires_grad=True 해줌\n",
    "# weight\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "y = w**2\n",
    "z = 10*y +25\n",
    "# w에 관한 gradient 구함\n",
    "z.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z에 관한 W의 gradient에, w값을 대입한 속성. 미분한 뒤 w 대입\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 편미분 autograd\n",
    "- backward(gradient = tensor) \n",
    "- where tensor dim 크기: 미분할 변수 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.,3.], requires_grad= True)\n",
    "b = torch.tensor([6.,4.], requires_grad= True)\n",
    "Q = 3*a**3 - b**2\n",
    "# 미분할 변수의 갯수 = external_grad 의 dim 크기\n",
    "# external_grad = dQ/dQ, 즉 1 이어야 한다.\n",
    "# 자기 자신에 대한 미분계수를 받으며 역전파(backward)시작하는 것 표현\n",
    "external_grad = torch.tensor([1.,1])\n",
    "Q.backward(gradient = external_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 36., 162.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 미분값\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12., -16.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b 미분값\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra_repr()\n",
    "- To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.\n",
    "- module_repr로 모델출력했을 때, 모듈(extra repr) 처럼 추가 repr을 달고싶을 때 사용\n",
    "- 모듈의 method로 넣어줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "    def forward(self, x):\n",
    "        x = x * 2\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'name=duck'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2004fe03f436a9a81b9c95e439c4add18d0165a64bc8b11e0efabfe79c313a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
